---
title: Probability Densities
date: 2024-03-16 19:47:00 +09:00
categories: [Statistics, Mathematical Statistics]
math: true
pin: true
tags:
  [
    probability,
    mathematical statistics,
    densities,
    expectation,
    variance
  ]
---


# Densities <sub>밀도 함수</sub>
어떤 distribution을 따르는 random variable $X$를 분석하기 가장 쉬운 방법은 $X$의 density가 존재한다면 이를 활용하는 것이다. Density가 존재하면 expectation<sub>기댓값</sub> 등의 대표적 통계 수치는 물론이고 나중에 다룰 likelihood<sub>가능도</sub>등을 분석할 수 있어 매우 좋은 inference<suB>추론</sub> 방법을 제공해준다. Density를 정의하기 전, 먼저 measure에서의 absolute continuity<sub>절대 연속성</sub>을 정의할 필요가 있다.

<br>

> **Definition 3.1** $P$와 $\mu$가 measurable space $(\mathcal{X}, \mathcal{A})$의 measure라고 하자. 만약 $\mu(A) = 0$인 모든 경우에 $P(A) = 0$이라면, $P$는 *$\mu$에 대해 absolutely continuous*<sub>절대 연속</sub>하다고 하며, $P \ll \mu$라고 표현한다.

<br>

위의 정의를 보면 대체 어느 부분에서 continuity와 관련이 있는지 의구심이 들 수 있다. 우리가 일반적으로 알고 있는 함수 의 absolute continuity정의를 다시 들고와 보자. $\mathbb{R}$의 interval $I$에 대하여 함수 $f:I\to\mathbb{R}$가 absolutely continuous할 조건은, 모든 $\epsilon>0$에 대하여 어떤 $\delta$가 존재하여 다음을 만족하는 임의의 pairwise disjoint인 $I$의 subinterval<sub>부분구간</sub> $(x_k, y_k),\ k=1, \cdots, N$에 대해

$$ \sum_{i=1}^N (y_i - x_i) < \delta$$

다음을 만족하는 성질이다.

$$\sum_{i=1}^N \vert f(y_i) - f(x_i)\vert\lt\epsilon$$

위의 상황에서 $A$가 subinterval들의 union $\bigcup_i (x_i, y_i)$으로 잡아두자. 또, $\mu$가 Lebesgue measure라고 하고, $P$는 $f$를 통해 정의된 measure라고 하자. 그러면 $f$의 absolute continuity를 다음과 같이 다시 표현할 수 있을 것이다.

$$\forall\epsilon,\ \exists\delta \quad\text{s.t.}\quad \mu(A)<\delta\ \Rightarrow P(A) <\epsilon$$ 

위 명제는 **Definition 3.1**과 상당히 비슷하다고 볼 수 있다. 그러므로 그 정의를 따라가다 보면, $P$는 $\mu$를 기저로 깔고 absolute continuity를 가지고 있음을 직관적으로 알 수 있다. 

이제 density를 정의할 준비가 완료되었다. 

<br>

> **Theorem 3.2 (Randon-Nikodym)** 만약 finite measure $P$가 $\sigma$-finite measure $\mu$에 대하여 absolute continuous할 때, 다음을 만족하는 non-negative function $f$가 존재한다.
>
> $$P(A) = \int_A f\,d\mu $$

<br>

그러한 $f$를 Radom-Nikodym derivative<sub>라돈 니코딤 도함수</sub>라고 하거나 density라고 한다. Notation으로는 다음을 자주 활용한다.

$$ f = \frac{dP}{d\mu} $$

Density는 유일하게 결정되지 않지만, 만약 두 개의 density $f$와 $g$를 찾게 되면 이 둘은 $f=g,\ a.e.$일 것이다. 그러므로 measure theory 관점에서는 그냥 같다고도 볼 수 있다.

만약 random variable $X$가 distribution $P_X$를 따른다면, 즉 $X\sim P_X$이면, 그리고 $P_X\ll \mu$이라면, $p = dP_X/d\mu$가 존재할 것이다. 이러한 경우 앞으로 $X$는 $\mu$에 대하여 density $p$를 가진다고 표현할 것이다.

<br>

> **Example 3.3** 만약 random variable $X$가 $\mathbb{R}$에 대한 Lebesgue measure에 대하여 density $p$를 가질 때, $X$ 혹은 $P_X$가 density $p$에 대하여 *absolutely continuous*하다고 말한다. 또, $X$를 *absolutely continuous random variable*이라고 부를 것이다. $X$가 absolutely continuous하다면 Radon-Nikodym 정리에 의하면,
>
> $$F_X(x) = P(X\le x) = P_X((-\infty, x]) = \int_{-\infty}^x p\,d\mu = \int_{-\infty}^x p(u)\,du $$
>
> 가 성립한다. 이 뜻은 적절한 regulation 조건 하에서는 fundamental theorem of calculus<sub>미적분학의 기본정리</sub>에 따라 $p$를 $p(x) = F_x'(x)$, 즉 cdf $F_X$를 미분하여 얻을 수 있다.

<br>

> **Example 3.4** $\mathbb{R}$의 부분집합으로써 countable set $\mathcal{X}_0$이 주어졌다. 모든 Borel set$B$에 대해 $\mu$를 다음과 같이 정의하자.
>
> $$\mu(B) = \# (\mathcal{X}_0\cap B)$$ 
>
> 이러한 measure는 간추려서 *$\mathcal{X}_0$의 counting measure*라고 말한다. $X$는 다음 조건을 가진 distribution을 따르는 임의의 random variable이라고 하자.
>
> $$P(X\in\mathcal{X}_0) = P_X(\mathcal{X}_0) = 1 $$
>
> 이러한 경우, $X$를 *discrete random variable*<sub>이산확률변수</sub>이라고 한다. 
>
> 만약 $N$이 $\mu$에 대한 null set이라면, $\mu$의 정의에 의해 $\\# (N\cap\mathcal{X}_0) = 0$이라는 의미이기에 결국 $N\cap\mathcal{X}_0 =\emptyset$이고 $N\subset \mathcal{X}_0^c$이다. 그러면, 
>
> $$P_X(N) = P(X\in N) \le P(X\in\mathcal{X}_0^c)=0$$
>
> 이므로 $P_X\ll \mu$임을 증명했다. 이때 $P_X$의 $\mu$에 대한 density $p$는 다음을 만족하게끔 찾으면 된다.
>
> $$ P_X(A) = \int_A p\,d\mu = \int p\cdot 1_A\,d\mu = \sum_{x\in\mathcal{X}_0} p(x)1_A(x)$$
>
> 만약 $A = \\{y\\}$으로 singleton으로 두면, 위의 식에 의하여
>
> $$P_X(\{y\}) = P(X = y) = p(y) $$
>
> 가 됨을 알 수 있다. 이 조건을 만족하는 density $p$는 discrete random variable에서 관찰할 수 있으며, 이때의 $P$를 *mass function*<sub>질량 함수</sub>라고 부른다. 만약 $y\not\in\mathcal{X}_0$이면, $p(y)$로 아무 값을 지정해도 상관없으나, 통상적으로는 $p(y)=0$이라고 둔다.

<br>

> **Example 3.5** 주어진 $\mathcal{E} = (0, 1)$에 대하여 $\mathcal{B}$는 $\mathcal{E}$의 Borel subset이라고 하자. 또, $A\in\mathcal{B}$에 대하여 $P(A)$는 $A$의 길이라고 정의하자. 그러면 $P$는 probability measure가 됨을 알 수 있는데, 이 상황에서는 $(0, 1)$ 범위 내에서 길이에 동등하게 비례하는 probability measure 값을 주므로 *uniform probability measure*<sub>균등확률측도</sub>라고 한다. 
>
> 하지만, 모종의 이유로 우리는 $1/2$ 이상의 값을 가지는 데이터를 제대로 측정하지 못하고 $1/2$으로만 표기한다고 하자. 이 데이터를 random variable $X$로 잡으면 $X$는 다음과 같이 정의할 수 있다.
>
> $$X(e) = \min\{e, 1/2\} $$
>
> $\mu$를 $\mathbb{R}$에서의 Lebesgue measure와 $\mathcal{X}_0 = \\{1/2\\}$의 counting measure의 합이라고 하자. 그러면 $X$의 distirbution $P_X$는 $\mu$에 대하여 absolutely continuous하다. 그 이유를 알아보자.
> 
> 먼저 $\mu = \lambda + \delta$라고 하자. 이때,  $\lambda$는 $\mathcal{R}$의 Lebesgue measure, $\delta$는 $\mathcal{X}_0$의 counting measure이다. $\mu(A)= 0$인 $A$에 대하여 우리는 $\lambda(A) = 0$이고 $\delta(A) = 0$임을 알 수 있다. 또,
>
> $$
> \{X\in A\} = \{e\in X^{-1}(A)\}=\begin{cases}
A\cap (0, 1/2) & 1/2\not\in A \\
[A\cap(0, 1/2)]\cup[1/2, 1) & 1/2\in A
\end{cases}
> $$
>
> 임에 따라, 
>
> $$P_X(A) = P(X\in A) = P(A\cap (0, 1/2)) + P([1/2, 1))\cdot 1_A(1/2) = \lambda(A\cap (0, 1/2)) + \frac{1}{2}\delta(\{1/2\}\cap A) = 0 $$
>
> 이므로 $P_X\ll \mu$임이 밝혀졌다. 이를 이용하여 더 나아가 density도 구해볼 수 있다.
>
> $$P_X(A) = \int f 1_A\,d(\lambda+\delta) = \int f1_A\,d\lambda + \int f1_A\,d\delta = \lambda(A\cap(0, 1/2)) + \frac{1}{2}\delta(\{1/2\}\cap A) $$
>
> 이 만족되어야 하므로, $f = 1\_{(0, 1/2)} + 1/2\cdot 1\_{\\{1/2\\}}$임을 알 수 있다. 이는 다음도 만족하기 때문이기도 하다.
>
> $$\lambda(\{1/2\}\cap A) =0,\ \ \delta((0, 1/2)\cap A) = 0 $$

<br>

위의 예시는 전형적인 continuous와 discrete 특성을 모두 가진 random variable의 예이다. 적당한 기저 measure를 통해 우리는 퉁분히 density를 구할 수 있음을 보았다.

<br>

> **Definition 3.6** Measurable space $(\mathcal{X}, \mathcal{A})$의 두 measure $\mu$와 $\nu$에 대하여 $\mu(A)=0$이고 $\nu(A^c)=0$인 $A\in\mathcal{A}$가 존재하면 $\mu$와 $\nu$는 mutually singular<sub>(??)</sub>하다고 한다. 기호로 $\mu\perp\nu$라고 표현한다.

<br>

위 정의는 absolute continuity와는 다른 느낌의 measure 간 관계를 보여준다. $\mu$는 마치 집합 $A$에서 지내는 느낌이지만 $\nu$는 $A$가 아닌 집합에서 지내는 느낌이다. 그러므로 두 measure는 서로 상반된 느낌을 지닌다. 

이제 주어진 measure과 하나는 absolute continuous하고 나머지 하나는 singular인 measure로 쪼개질 수 있음을 명시할 것이다.

<br>

> **Theorem 3.7 (Lebesgue's Decomposition Theorem)** Measurable space $(\mathcal{X}, \mathcal{A})$에서 정의된 $\sigma$-finite measure $\mu$와 $\nu$에 대하여 다음을 만족하는 두 $\sigma$-finite measure $\nu_0$와 $\nu_1$가 유일하게 존재한다.
>
> * $\nu = \nu_0+\nu_1$
> * $\nu_0\ll \mu$ 
> * $\nu_1\perp \mu$

<br>

$\nu_0$는 $\mu$에 대하여 absolute continuous하고, $\nu_1$은 $\mu$에 대하여 singular하다. 그리고 이 둘은 결국 $\nu$를 두 개로 쪼갠 것이다. 이 theorem<sub>정리</sub>의 증명을 다루진 않겠지만 Radon-Nikodym theorem의 증명에서 한 발만 더 나아가면 될 정도로 비슷하다.

<br>
<br>

# Expectation <sub>기댓값</sub>

Probability space $(\mathcal{E}, \mathcal{B}, P)$의 random variable $X$에 대하여 우리는 $X$의 expectation 혹은 expected value를 다음과 같이 정의한다.

$$ EX = \int X\,dP = \int X(e)\, dP(e)$$ 

하지만 위의 정의대로 expectation을 구하는 경우는 적은데, 아무래도 일반적인 산수로 계산이 다소 힘들기 때문이다. 하지만 distribution이 주어진다면 우리는 비로소 산수가 편한 형태의 공식을 얻게 된다. 

$$ EX = \int x\, dP_X(x) $$

그리고 $Y = f(X)$의 expectation은 다음과 같이 정의한다.

$$ EY = E f(X) = \int f\, dP_X = \int f(x)\, dP_X(x)$$

$P_X$에 대한 integral은 주로 더 편리하게 density를 빌려 계산된다. 만약 $P_X$가 $\mu$에 대하여 density $p$를 가지고 있다면,

$$\int f\, dP_X = \int fp\, d\mu$$

로 계산이 가능하고 $\mu$가 Lebesgue measure등과 같이 적분이 편리한 measure라면 적분 계산이 훨씬 수월하다. 결국 $dP_X$는 $p\,d\mu$로 자유롭게 치환이 가능하며, 때문에 $p = dP_X/d\mu$로 표현한 notation이 훨씬 자연스러워 보인다. 

> **Example 3.8** 만약 $X$가 density $p$를 가지는 absolutely continuous random variable이라면, 
>
> $$EX = \int x\, dP_X(x) = \int xp(x)\, d\mu(x)$$
>
> 로 계산된다. 이때 $\mu$는 Lebesgue measure로써, 그냥 $x$로 적분하는 것처럼 취급할 수 있다. 
> 
> 또, countable set $\mathcal{X}_0$에 대하여 $P(X\in\mathcal{X}_0)=1$인 discrete random variable $X$를 고려하자. $\mu$가 $\mathcal{X}_0$ 위의 counting measure라면,
>
> $$ EX = \int x\, dP_X(x) = \int xp(x)\, d\mu(x) = \sum_{x\in\mathcal{X}_0}xp(x) $$
>
> 로 간단하게 계산이 가능하다.

<br>

위 example에서는 random variable이 discrete할 때와 continuous 할 때 각각 expectation의 형태가 어떻게 도출되는지 살펴보았다.

Expectation은 linearity<sub>선형성</sub> 또한 갖추고 있다. 0이 아닌 constants $a, b$에 대하여 

$$ E(aX+bY) = aEX + bEY$$

가 성립한다. 이때, 우변에서는 $\infty-\infty$꼴이 되지 않음이 보장되어야 한다. 또 하나 중요한 property<sub>성질</sub>은 $X<Y\, a.e.\, P$ 일 때 $EX<EY$가 된다는 점이다. 즉, random variable의 order를 expectation은 그대로 유지시켜준다. 만약 $X\le P\,a.e.\, P$라면 $EX\le EY$일 텐데, 이때 등호는 $X = Y\, a.e.\,P$일 때만 성립한다. 이는 linearity와 integration property로 유도가 가능하다.

$X$의 variance<sub>분산</sub>는 분포의 퍼짐 정도를 나타내는 수치로써 다음과 같이 정의된다.

$$ Var(X) = E(X-EX)^2 $$

그러므로 만약에 $X$가 continuous이며 density $p$를 가지면

$$Var(X) = \int (x - EX)^2p(x)\, dx $$

이고, 만약 $X$가 countable set $\mathcal{X}_0$에 대하여 discrete이고 mass function $p$를 가지고 있다면

$$Var(X) = \sum_{x\in\mathcal{X}_0} (x - EX)^2p(x)$$

이다. 다음 공식은 expectation을 직접 계산할 때 조금 더 간단하게 할 수 있도록 도와준다.

$$ Var(X) = E(X^2 - 2X\,EX +(EX)^2) = EX^2 - (EX)^2 $$

두 random variable $X, Y$ 간 관계, 특히 선형 관계를 잡아내는 covariance는 다음과 같이 정의된다.

$$ Cov(X, Y) = E(X - EX)(Y - EY) $$

여기서 $X = Y$로 두게 되면 $Cov(X, X) = Var(X)$이 됨을 알 수 있다. Covariance도 마찬가지로 계산이 편리할 수 있도록 공식을 유도할 수 있다.

$$Cov(X, Y) = E(XY - X\,EY-Y\,EX +(EX)(EY))=EXY - (EX)(EY)$$

하지만 covariance 값 자체로는 두 random variable의 연관성을 판단하기 힘들 수도 있는데, 이는 covariance가 $X$와 $Y$ 각각의 단위에도 크게 영향을 미치기 때문이다. 따라서 normalize가 필요하며, 이를 취해 만든 값이 correlation<sub>상관계수</sub>이다.

$$ Cor(X, Y) = \frac{Cov(X, Y)}{[Var(X)\,Var(Y)]^{1/2}}$$

Correlation은 항상 $[-1, 1]$의 범위에 머물기 때문에 상관의 정도를 비교하기 쉽다. 또, 만약 correlation이 1과 -1이라면 두 random variable은 완벽한 linear 관계를 가지고 있음을 시사한다.

만약 $X:\mathcal{E}\to\mathbb{R}^n$이면, $X$는 random vector<sub>확률벡터</sub>라고 부른다. 이는 결국 random variable $X_1, \cdots, X_n$에 대하여

$$ X(e) = \begin{pmatrix} X_1(e) \\ \vdots \\ X_n(e)\end{pmatrix}, \quad e\in\mathcal{E} $$

으로 정의할 수 있다. Random vector 역시 Borel set $B$에 대하여 

$$P_X(B) = P(X\in B)$$

으로 distribution $P_X$가 정의되며, $P_X$가 $\mathbb{R}^n$의 Lebesgue measure $\mu$에 대하여 absolutely continuous하여 density $p$를 가진다면, density로 확률을 구할 때 다음과 같이 구할 수 있을 것이다.

$$ P(X\in B) = \int_Bp\,d\mu = \underset{B}{\int\cdots\int}p(x)\,dx $$

Expectation도 마찬가지로 다음과 같이 정의된다.

$$EX = \begin{pmatrix} EX_1 \\ \vdots \\ EX_n\end{pmatrix}$$ 

Variance는 이야기가 약간 달라지는데, random vector의 variance는 일단 covariance라고 통칭하여 부르며 이는 random matrix<sub>확률행렬</sub>라고 한다. Random vector $X$의 covariance는 다음과 같이 정의된다.

$$[Cov(X)]_{ij} = Cov(X_i, X_j)$$

즉, matrix의 $(i, j)$ 자리에는 $X_i$와 $X_j$의 covariance가 담겨있다. Diagonal element<sub>대각원소</sub>들이 결국 $X_1, \cdots, X_n$의 variance가 될 것이다. 또, $\mu = EX$에 대하여

$$ Cov(X_i, X_j) = E(X_i-\mu_i)(X_j - \mu_j) = E[(X-\mu)(X-\mu)^T]_{ij}$$

이므로

$$Cov(X) = E(X-\mu)(X-\mu)^T$$

임을 알 수 있다. 계산하기 편리한 공식 역시 유도가 가능한데 다음과 같다.

$$Cov(X) = EXX^T - (EX)(EX)^T $$

만약 random vector들에 각각 상수 행렬 $A, B$에 곱하였을 때 이 둘의 covariance는

$$Cov(AX, BY) = E(AX-EAX)(BY-EBY)^T = EA(X-EX)(Y-BY)^TB^T = ACov(X, Y)B^T$$

이다.

<br>
<br>

# Independence <sub>독립성</sub>
두 random variable 간의 independence를 정의하기 전 우리는 두 random variable이 joint되었을 때 어떤 measure를 활용할지부터 정해야 한다. 그래서 등장한 것이 *product measure*<sub>(??)</sub>이다.

<br>

> **Definition 3.9** Measure space $(\mathcal{X}, \mathcal{A}, \mu)$와 $(\mathcal{Y}, \mathcal{B}, \nu)$에 대하여 다음을 만족하는 measure $\mu\times\nu$가 $(\mathcal{X}\times\mathcal{Y}, \mathcal{A}\vee\mathcal{B})$위에 *유일하게* 존재한다. $A\in\mathcal{A}$와 $B\in\mathcal{B}$에 대하여
>
> $$(\mu\times\nu)(A\times B) = \mu(A)\nu(B)$$
>
> 이때, $\mathcal{A}\vee\mathcal{B}$는 $A\in\mathcal{A}$와 $B\in\mathcal{B}$에 대하여 $A\times B$를 모두 포함하는 가장 작은 $\sigma$-algebra를 의미한다.
>

<br>

가장 직관적인 예로는 만약 $\mu$와 $\nu$가 각각 $\mathbb{R}^n$과 $\mathbb{R}^m$의 Lebesgue measure일 때, $\mu\times\nu$는 $\mathbb{R}^{n+m}$ 위의 Lebesgue measure이다. 그렇다면 $\mu\times\nu$ 위에서의 integral은 어떻게 계산될까? 여기에서 등장하는 것이 바로 Fubini's theorem이다.

<br>

> **Theorem 3.10 (Fubini)** 만약 $f\ge0$이라면,
>
> $$\int f\,d(\mu\times\nu) = \int\left[\int f(x,y)\,d\nu(y)\right]\,d\mu(x) = \int\left[\int f(x, y)\,d\mu(x)\right]\,d\nu(y)$$
>
> 이다. $f\ge0$ 이라는 제한을 풀면, 만약
>
> $$\int \vert f\vert\,d(\mu\times\nu)<\infty$$
>
> 를 만족하면 위의 정리는 똑같이 성립한다.

<br>

위의 정리는 오로지 $\mu\times\nu$와 관련된 probabililty를 계산할 때만 쓰이는 것이 아니라, $A\in\mathcal{A}$와 $B\in\mathcal{B}$의 Cartesian product<sub>카르테시안 곱</sub> $A\times B$이 아닌 집합 $S$에 대하여 $(\mu\times\nu)(S)$를 계산할 수 있는 방법을 제공한다. 바로 $f = 1_S$라고 두면 되니 말이다.

이제 independence를 정의할 준비가 완료되었다.

<br>

> **Definition 3.11** 두 random vector $X\in\mathbb{R}^n$과 $Y\in\mathbb{R}^m$가 *independent*<sub>독립</sub>이라는 것은 Borel set $A$와 $B$에 대하여
>
> $$ P(X\in A, Y\in B) = P(X\in A)P(Y\in B)$$
>
> 를 만족한다는 의미이다.

<br>

만약 $Z = \begin{pmatrix} X \\\\ Y\end{pmatrix}$라면, $Z\in A\times B$인 것은 $X\in A$dhk $Y\in B$인 것과 equivalent하다. 두 random vector $X$와 $Y$가 independent하다고 하자. 그러면 $Z$의 distribution $P_Z$는 다음과 같이 만들어질 것이다.

$$P_Z(A\times B) = P_X(A)P_Y(B)$$

즉, $P_Z = P_X\times P_Y$라는 의미이다. 만약 $X$와 $Y$ 각각 $\mu$와 $\nu$에 대하여 density $p_X$와 $p_Y$가 있을 때 $p_Z$의 density는 $\mu\times\nu$에 대하여 $p_Xp_Y$이다. 이렇게 여러 random variable 혹은 vector가 이어붙여져 한 번에 distribution와 density를 찾았는데, 이를 $P_Z$를 joint distribution, $p_Z$는 joint density라고 한다.

이러한 성질은 $X_1, \cdots, X_n$과 같이 $n$개의 random variable에 대해서도 동일하게 성립한다. 특히 이들이 independent하다는 것은 모든 Borel set $B_1, \cdots, B_n$에 대하여

$$P(X_1\in B_1, \cdots, X_n\in B_n) = P(X_1\in B_1)\times\cdots\times P(X_n\in B_n)$$

하다는 것이다. 또 이들을 vectorize한 것을 $Z$라고 할 때, $P_Z = P\_{X_1}\times\cdots\times P\_{X_n}$이라고 한다.

우리는 마지막으로 중요한 property를 살펴본다.

<br>

> **Proposition 3.12** 만약 $X_1, \cdots X_n$가 independent random vector이고, $f_1, \cdots, f_n$이 measurable function일 때, $f_1(X_1), \cdots, f_n(X_n)$는 여전히 independent하다.

<br>
<br>

---
다음 포스트에서는 exponential family<sub>지수족</sub>과 이와 관련하여 differential identity, 즉 differentiate<sub>미분</sub>했을 때 나타나는 성질들을 살펴볼 것이다.
