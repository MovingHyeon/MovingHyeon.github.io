---
title: Maximum Likelihood Estimator(최대가능도추정량)
date: 2024-04-27 00:54:00 +09:00
categories: [Statistics, Mathematical Statistics]
math: true
pin: true
tags:
  [
    probability,
    mathematical statistics,
    likelihood,
    maximum likelihood estimator,
    MLE,
    estimator,
    large sample theory
  ]
---

Maximum likelihood estimator는 frequentist가 중요하게 여기는 또 다른 유형의 estimator로 likelihood<sub>가능도</sub>를 최대화하여 얻어지는 estimator이다. 이 estimator는 우리가 model로 가정한 probability distribution이 어느 parameter $\theta$ 값이 주어진 data를 가장 잘 설명할 수 있는지를 찾는 것과 같다. 이를 찾는 방법으로써 likelihood를 최대화하는 parameter의 값을 찾는 방식을 사용한다. Likelihood는 이전 글에서도 몇 번 언급했지만, data의 joint distribution을 parameter의 함수로 다시금 바라본 함수이다. Likelihood가 결국 설명하는 것은, 주어진 parameter $\theta$에 대하여 우리가 가정한 probability distribution $P\_{\theta}$ 아래에서 이미 수집된 data가 그와같이 나올 가능성, 경향성 정도를 의미한다. 결국 Likelihood를 최대화하는 $\theta$를 찾겠다는 것은 data와 가장 잘 어울리는 probability distribution의 $\theta$값을 찾겠다는 것을 의미한다. 해당하는 $\theta$ 값은 true value의 $\theta$를 estimate하기 위한 performance 좋은 estimator로써 자리매김하는데, 이 estimator의 이름이 바로 maximum liklihood estimator<sub>최대가능도추정량 </sn/>이다. 상당히 직관적인 철학을 바탕으로 정의된 듯한 maximum likelihood estimator가 실질적으로 효율적이고 성능이 좋은 estimator인지 알아보기 위해서는 조금 더 많은 과정을 거쳐야 한다. 하지만, 적어도 이렇게 정의한 estimator는 우리의 믿음과 직관으로 보았을 땐 상당히 타당한 과정에 입각하여 정의되었고, 좋은 성질을 가질 것이라 기대할만도 하다.

<br>
<br>

먼저 data $X$의 joint density를 $p\_{\theta}$라고 하자. 만약 data가 i.i.d.였다면 density $p\_{\theta}$는 각 data $X\_i$ joint density의 전체 곱으로 나타내어질 것이다. 이 density함수는 아직 data $x$의 함수이다. 하지만 다른 관점으로 $p\_{\theta}$를 $\theta$의 함수로 나타낼 수 있고, 이를 likelihood라고 한다.

$$L(\theta) = p\_{\theta}(X)$$

그리고 $L(\cdot)$를 최대화하는 $\hat{\theta} = \hat{\theta}(X)$를 $\theta$의 *maximum likelihood estimator(MLE)*라고 한다. 그리고 $g(\theta)$의 MLE는 $g(\hat{\theta})$로 둔다(이렇게 두어도 원래 MLE 정의와 충돌이 딱히 생기지 않는다). 하지만 likelihood $L(\cdot)$는 주로 곱으로 이루어진 항이 많기 때문에 미분이 어렵다. MLE를 구하기 위해서 logarithm를 씌운 *log-likelihood*<sub>로그가능도함수</sub>를 활용하면 미분하기 매우 수월하다.

$$l(\theta) = \log L(\theta)$$

이제 exponential family를 density로 가지는 data의 MLE 형태를 살펴보자.

<br>
<br>

> **Example 20.1** Data $X$의 density가 canonical one-parameter exponential family로부터 비롯되었을 때, 우리는 density를 다음과 같이 작성할 수 있다.
>
> $$ p_{\eta}(x) = \exp\{\eta T(x) -A(\eta)\}h(x) $$
>
> MLE를 구하기 위해서 log-likelihood를 살펴보자.
>
> $$ l(\eta) = \log p_{\teta}(X) = \eta T - A(\eta) +\log h(X) $$
>
> 우리는 다음을 얻는다.
>
> $$ 
> \begin{align*}
> l'(\eta) &= T - A'(\eta) \\
> l''(\teta) = -A''(\teta) - Var_{\eta}(T)\lt 0 \\
> \end{align*}
> $$
>
> 이계도함수가 항상 음수이므로 local minimum이 unique하여 global minimum이 된다. 그러므로 $A'$의 inverse를 $\psi$라고 두면 $\eta$의 MLE는
>
> $$\hat{\eta} = \psi(T) $$
>
> 이다.
>
> 이제 data가 $n$개의 i.i.d. random sample $X\_1, \cdots, X\_n$으로 이루어져 있고, density $p_{\eta}$를 가지고 있을 때 전체 data $X = (X\_1, \cdots, X\_n)$의 joint density는 $\prod\_{i=1}^n p\_{\eta}(x\_i)$이고 log-likelihood는
>
> $$ l(\eta) = \eta\sum_{i=1}^n T(X_i) - nA(\eta) + \sum_{i=1}^n \log h(X_i)$$
>
> 이다. 위와 마찬가지 방법으로 MLE를 구하면
>
> $$ \hat{\eta} = \psi(\overline{T}) \quad\text{where}\quad \overline{T} = \frac{1}{n}\sum_{i=1}^n T(X_i) $$
>
> 이다. 이것이 다른 한편으로 보여주는 것은 $E\_{\eta} T(X\_i) = A'(\eta)$의 MLE가
>
> $$ A'(\hat{eta}) = \overline{T}$$
>
> 즉 그저 $T$의 sample mean이라는 것이다. 
>
> 한 가지 더 관찰할 수 있는 점은 $\eta$의 MLE가 complete sufficient statistic $T$의 함수라는 것이다. 그러므로 $\eta$의 MLE 역시 UMVUE임을 알 수 있다. 하지만 일반적인 경우에서는 항상 UMVUE는 아니고 심지어 bias를 가지고 있을 수도 있다.
>
> 이 상황에서 MLE가 i.i.d. sample들의 mean의 함수이기 떄문에 CLT와 delta method를 이용하여 limiting distribution를 알 수 있다. 먼저 $Var_{\eta}(T) = A''(\eta)$이기 때문에 CLT에 의하여
>
> $$\sqrt{n}(\overline{T} - A'(\eta)) \leadsto N(0, A''(\eta)) $$
>
> 이다. 이제 $\psi(\cdot)$로 delta method를 적용해야 한다. $\psi$의 도함수는 inverse function derivative를 구하는 방법을 참고하면
>
> $$ \psi' = \frac{1}{A''(\psi(\cdot))} $$
>
> 이다. 그러므로 $\psi'(A'(\eta)) = 1/A''(\eta)$이고, delta method에 의해
>
> $$\sqrt{n}(\hat{\eta} - \eta) \leadsto N(0, 1/A''(\eta)) $$
>
> 이다. 한편, 각 data $X\_i$가 전달하는 Fisher information은
>
> $$ I(\eta) = Var_{\eta}\left(\frac{\partial}{\partial\eta}\log p_{\eta}(X_i)\right) = Var_{\eta}(T(X_i)) = A''(\eta) $$
>
> 이므로, 전체 data에 대한 Fisher information은 $J(\theta) = nI(\theta) = nA''(\eta)$이며,  Cramer-Rao bound에 의하여 $\eta$의 unbiased estimator $\tilde{\eta}$에 대하여
>
> $$ Var_{\eta}(\tilde{\eta}) \ge \frac{1}{nA''(\eta)}$$
>
> 임에 따라
>
> $$ Var_{\eta}(\sqrt{n}(\tilde{\eta} - \eta)) = nVar_{\eta}(\tilde{\eta})\ge \frac{1}{A''(\eta)}$$
>
> 가 성립한다. 그러므로 MLE $\hat{\eta}$의 variance는 $n\to\infty$임에 따라 가장 최소화됨을 알 수 있다. 즉, asymptotic<sub>점근적으로</sub>하게 Cramer-Rao bound에 도달한다. 이러한 형태의 estimator들을 *asymptotically efficient*하다고 한다.

<br>
<br>

MLE의 limiting distribution은 굉장한 성질이 있기에 나중에 여러 편에 걸쳐서 소개하게 될 것이다. 또한 asymptotically efficient한 estimator들의 property를 다루기도 쉽지 않은 편이라 이 시리즈의 뒷편에서 다루도록 한다.

<br>
<br>
<br>

---
다음 편에서는 median이 estimator로써의 역할을 하게 되면 어떠한 성질들이 있는지 살펴본다. 특히 median의 limiting distribution과 performance를 정량적으로 살펴보게 될 것이다.