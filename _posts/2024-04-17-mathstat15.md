---
title: Proof of Factorization Theorem
date: 2024-04-16 00:05:00 +09:00
categories: [Statistics, Mathematical Statistics]
math: true
pin: true
tags:
  [
    probability,
    mathematical statistics,
    conditional distributuion,
    product measure,
    product algebra,
    factorization theorem,
    factorization theorem proof
  ]
---

<br>
<br>

오류 제보: ac585258 (at) gmail (dot) com

<br>
<br>
<br>

<center><div style="width:48%"><i>
I am too familiar with the manner in which actual data are met with the suggestion that other data, if they were collected, might show something else to believe it to have any value as an argument. "Statistics on the table, please," can be my sole reply. <br>

-- Karl Pearson, 1910
</i></div></center>

<br>
<br>

이번 post에서는 factorization theorem을 증명해본다. 이 theorem의 증명을 지금까지 미룬 이유는 증명할 때 conditional distribution의 정의를 활용해야 하기 때문이다. 직접적인 정의를 활용하는 이유는 대부분의 경우에서 sufficient statistics $T$와 data $X$의 joint density를 찾으려고 해도, 그 어떤 product measure 하에서도 이들의 joint density가 없기 때문이다. 증명이 상당히 technical하기 때문에 굳이 모든 것을 처음에 이해할 필요는 없다.

<br>
<br>

# Discrete Case

Measure theory를 이용하여 일반적으로 증명하기 전에 먼저 discrete case로 간단히 증명해보고 증명의 방향성을 잡아보자. 일반적인 경우와 discrete할 때의 증명은 같은 맥락을 따라가기 때문에 discrete case의 증명을 먼저 살펴보면 이해가 훨씬 수월하리라 생각한다.

<br>

먼저 $X$의 mass function $p_{\theta}(x) = g_{\theta}(T(x))h(x)$가 성립한다고 가정하자. 그리고 $q_{\theta}(t)$를 $T$의 mass function이라고 정의하자. 그리고 $A_t := T^{-1}(\\{t\\}) = \\{y: T(y) = t\\}$라고 하자. 그러면

$$q_{\theta}(t) = P_{\theta}(T = t) = \sum_{y\in A_t} p_{\theta}(y) = \sum_{y\in A_t} g_{\theta}(T(y))h(y) = g_{\theta}(t)\sum_{y\in A_t} h(y) $$

가 성립하고, 따라서 $X \vert T$의 mass function은 다음과 같이 도출된다.

$$
\begin{align*}
\frac{p_{\theta}(x)}{q_{\theta}(T(x))} &= \frac{g_{\theta}(T(x))h(x)}{g_{\theta}(T(x))}\\
&= \frac{g_{\theta}(T(x))h(x)}{g_{\theta}(T(x)) \cdot \sum_{y\in A_{T(x)}} h(y)}\\
&= \frac{h(x)}{\sum_{y\in A_{T(x)}}h(y)}
\end{align*}
$$

$X\vert T$의 mass function이 $\theta$와 무관함을 알 수 있으므로 $T$는 sufficient함이 증명되었다.

이제 그 converse를 증명하자. $T$가 sufficient하다고 가정한다. 그리고 $g_{\theta}(t) := P_{\theta}(T = t)$는 $P_{\theta}$에서의 mass function, $h(x) := P(X = x\vert T(x) = t(x))$라고 두자. $T$가 sufficient 함에 따라 $h$는 $\theta$와 무관하다. $T$는 어쨌든 $X$의 함수이기 때문에 $P_{\theta}(X = x) = P_{\theta}(X = x, T = t(x))$가 성립한다. 이에 따라 $t = t(x)$에 대하여

$$
\begin{align*}
p_{\theta}(x) &= P_{\theta} (X = x) \\
&= P_{\theta}(X = x, T = t(x)) \\
&= P_{\theta}(T = t) P_{\theta}(X = x\vert T = t) \\
&= g_{\theta}(T(x)) h(x) 
\end{align*}
$$

이렇게 factorization theorem이 $X$가 discrete한 경우에 대해서 모두 증명되었다. 

<br>
<br>

# In Measure Theoretic Perspective

이제 더욱 일반적인 상황에 대하여 factorization theorem을 증명할 것이다. 하지만 전체적인 outline은 discrete case에서의 증명과 흡사하다.

Factorization theorem을 증명하기 앞서 가장 크게 직면하는 문제는 data $X$와 sufficient statistics $T$의 joint density가 없는 경우가 대부분이라는 점이다. 그 이유는 $T$가 $X$의 함수라는 부분에서 비롯된다. 둘 사이의 함수 관계가 있다고 하여 무조건적으로 joint density가 없는 것은 아니지만, 가능한 $(x, T(x))$ 값들의 집합 $\\{(x, T(x)): x\in\mathbb{R}\\}$이 product measure 입장에서는 0이지만 실제 probability measure로 볼 때는 0이 아닌 경우가 많기 때문이다. 일례를 들어, 집합 $\\{(x, T(x)): x\in\mathbb{R}\\}$도 $\mathbb{R}$에서의 Lebesgue measure에 대해서는 1이지만, $\mathbb{R}^2$ 위에서의 product Lebesgue measure에 대해서는 그 집합은 곡선 정도로만 나타나므로 0이다. 이렇게 되면 probability measure가 그 product measure에 absolute continuous하지 않아지게 되어 density가 없다. 때문에 $X\vert T$의 conditional density를 구할 때도 joint density를 통해 구할 수 없고, conditional distribution의 수학적 정의를 따라가야만 한다. 

Conditional distribution으로 작업을 나아가기 위해서는 $\theta$와 무관하지만 model $\mathcal{P} = \\{P_{\theta}: \theta\in\Omega\\}$의 probability measure $P_{\theta}$와 equivalent한 probability measure가 필요하다. 즉, 

$$\mu(N) = 0 \Leftrightarrow P_{\theta}(N) = 0, \forall\theta\in\Omega $$

인 $\mu$를 잡을 것이다. 이러한 $\mu$ 존재할 조건은 model $\mathcal{P}$와 equivalent한 countable subfamily $\\{P_1, P_2, \cdots\\}$가 존재하면 된다. 그러면 적절한 constants $c_i$에 대하여 $\mu = \sum\_{i=1}^\infty c\_i P\_{\theta\_i}$로 작업하여 $\mu$를 $\mathcal{P}$와 equivalent하게 만들 수 있음이 알려져 있다. 이 부분까지는 증명으로 언급하진 않을 테지만, 만약 더 궁금하다면 Lehmann과 Romano의 매우 유명한 저서 *Testing Statistical Hypothesis(TSH)*의 appendix A 4장을 참고하라. 우리는 아무튼 그러한 probability measure $\mu$를 만들었다고 생각하고 다음을 정의한다. 기호가 난무하기 때문에 정확히 체크하자!!

* $P^\*$와 $E^\*$는 각각 $X\sim \mu$ 하에서의 probability와 expectation을 의미한다.
* $G^\*$와 $G\_{\theta}$는 각각 $X\sim \mu$와 $X\sim P_{\theta}$ 하에서의 $T(X)$의 marginal distribution이다.
* $Q$는 $X\sim\mu$ 아래에서의 $T$가 주어졌을 때 $X$의 conditional distribution이다. 즉, $X\vert T = t\sim Q\_t$.

조금 더 직관적인 설명을 시도해보려고 한다. 먼저 $\mu$는 $X$를 다룰 수 있는 아주 기초적인 probability measure이다. $X$의 distribution $P_{\theta}$는 모두 $\mu$에 대하여 density $p_{\theta}$를 가진다. $T$는 $X$에 대한 함수로써, $X$ 자체는 아니므로 $t$ 자체에 대한 기초적인 probability measure가 필요하다. 그것이 $G^\*$인 셈이다. 그리고 $X\sim P_{\theta}$ 아래에서 $T$는 $t$의 언어로 표현되는 probability distribution이 있어야 하고, 그것이 $G_{\theta}$이다. 마지막으로 $Q$는 $X\sim \mu$ 아래에서의 conditional distribution인데, 주어진 condition $T = t$가 고정되면 $Q_t$ 자체는 $x$에 대한 measure이다. 그리고 $X\sim P_{\theta}$ 하에서의 $X\vert T$ condtional distribution은 아직 표현을 하기엔 애매한 것이, 이 distribution이 $\theta$에 dependent한지 아닌지 모르기 때문이다. 이는 이후 증명과정에서 $X\sim P_{\theta}$ 아래에서의 conditional distribution을 새로 정의하면서 그 여부를 알아낼 것이다. 

먼저 $P_{\theta}$가 $\mu$에 대하여 다음의 density를 가진다고 가정하자.

$$p_{\theta}(x) = g_{\theta}(T(x)) h(x) $$

이를 이용하여 $T$의 $G^*$에 대한 density, 즉 $t$로 표현되는 density를 찾아본다. Discrete case에 대해서도 $T$에 대한 mass function을 먼저 탐구했던 것을 상기하라.

$$
\begin{align*}
E_{\theta}f(T) &= \int f(T(x))\,dP_{\theta}(x) = \int f(T(x))\cdot g_{\theta}(T(x))h(x)\,d\mu(x) \\
&= E^* f(T)g_{\theta}(T)h(X) \\
&= \iint f(t)\cdot g_{\theta}(t)h(x)\,dQ_t(x)\,dG^*(t) \\
&= \int f(t)\cdot g_{\theta}(t)w(t)\,dG^*(t) \quad \text{where}\quad w(t) = \int h(x)\,dQ_t(x) 
\end{align*}
$$

위의 과정을 통해서 $T$의 $G^*$에 대한 density는 $g_{\theta}\cdot w$임을 알았다. 즉,

$$\frac{dG_{\theta}}{dG^*} = g_{\theta}\cdot w $$

이제 probability distribution $\tilde{Q}_t$를 $Q_t$에 대해 density $h(x)/w(t)$를 갖도록 잡을 것이다. 그러면 이 probability measure는 주어진 Borel set $B$에 대하여 probability를 다음과 같이 계산할 것이다.

$$ \tilde{Q}_t(B) = \int_B \frac{h(x)}{w(t)}\,dQ_t(x) = \frac{\int_B h(x)\,dQ_t(x)}{\int h(x)\, dQ_t(x)}$$

번외로 $w(t) = 0$인 $t$에 대해서는 $\tilde{Q}_t$를 임의의 probability measure로 잡아도 좋다. 이제 $\tilde{Q}_t$가 어떤 역할을 하는지 알아보기 위해 $X$와 $T$의 joint를 알아보는 척(?) 그 expectation을 살펴보자.

$$
\begin{align*}
E_{\theta} f(X, T) &= \int f(x, t(x))\,dP_{\theta}(x) = \int f(x, t(x))p_{\theta}\,d\mu(x) = \int f(x, t(x))g_{\theta}(T(x))h(x)\,d\mu(x) \\
&= E^* f(X, T) g_{\theta}(T)h(X) \\
&= \iint f(x, t) g_{\theta}(t)h(x)\, dQ_t(x)\, dG^*(t) \\
&= \iint f(x, t)\cdot \frac{h(x)}{w(t)}\, dQ_t(x) \cdot g_{\theta}(t) w(t)\, dG^*(t) \\
&= \iint f(x, t)\, d\tilde{Q}_t(x)\,dG_{\theta}(t)
\end{align*}
$$

위의 결과가 보여주는 것은 이전 post에서 소개한 conditional distribution의 정의에 따라 $\tilde{Q}\_t$가 $X\sim P\_{\theta}$ 아래에서의 $X\vert T = t$의 conditional distribution라는 점이다. 그리고 우리가 $\tilde{Q}\_t$를 정의할 때 $\theta$에 관한 항이 없었음을 살펴보라. $T$는 따라서 sufficient함을 알 수 있다. 

이제 converse를 증명할 것이다. 증명에 들어가기 전 한 가지 정의해야 할 것이 있는데, 바로 *mixture distribution*<sub>혼합분포</sub>이다. 주어진 marginal probability distribution $G^\*$와 conditional distribution $Q$에 대하여, 이들의 mixture distribution $\hat{P}$는 모든 Borel set $B$에 대하여 다음과 같이 정의한다.

$$\hat{P}(B) = \int Q_t(B)\,dG^*(t) = \iint 1_B(x)\,dQ_t(x)\,dG^*(t) $$

$\hat{P}$는 $x$에 관한 measure이며, 기저 conditional distribution을 $t$의 기저 distribution에 따라 혼합한 probability distribution으로 볼 수 있다. $\mu$에 대하여 absolute contiuous 관계를 가져 density를 뱉을 수도 있는, 적당히 혼합된 기본적인 probabilty distribution이라고 직관적으로 받아들일 수 있겠다.

이제 $T$가 sufficient 함을 가정하자. 궁극적으로는 $p_{\theta}$가 decomposition이 됨을 증명할 것이다. $T$가 sufficient 하므로 $X\vert T$의 conditiona distribution $Q$는 $\theta$와 무관하다. 그리고 $g\_{\theta}$를 $X\sim P\_{\theta}$ 하에서의 $G^\*$에 대한 density라고 하자(혹은 $g\_{\theta} = dG\_{\theta}/dG^\*$). Density $g_{\theta}$가 존재하는 이유를 간단히 살펴보자. $G^*(N) = 0$에 대하여 $N_0 = T^{-1}(N)$으로 두면 $\mu(N_0) = 0$일 것이다. 그러므로

$$ G_{\theta}(N) = P_{\theta}(T\in N) = P_{\theta}(X\in N_0) = \int_{N_0} \, dP_{\theta} = \int_{N_0} p_{\theta}\,d\mu = 0$$

Density $p_{\theta}$를 좀 더 살펴보기 위해 임의의 Borel set $B$에 대한 $X$의 probability를 조금 더 헤쳐보자.

$$
\begin{align*}
P_{\theta}(X\in B) &= E_{\theta}P_{\theta} (X\in B \vert T) \\
&= E_{\theta} Q_T(B) \\
&= \int Q_t(B)\, dG_{\theta}(t) = \int Q_t(B) g_{\theta}\, dG^*(t)\\
&= \iint 1_B(x)\, dQ_t(x)\, g_{\theta}(t)\, dG^*(t) \\
&= \iint 1_B(x) g_{\theta}(T(x))\, dQ_t(x)\, dG^*(t) \\
&= \int_B g_{\theta}(T(x))\, d\hat{P}
\end{align*}
$$

위의 과정이 보여주는 것은 $P\_{\theta}$가 $\hat{P}$에 대하여 $g\_{\theta}\circ T$를 density로 갖는다는 것이다. 우리는 마지막으로 $\hat{P}\ll\mu$임을 증명하여 $\hat{P}$가 $\mu$에 대하여 어떤 density를 가지는지 구할 것이다. $\mu(N) = 0$이라고 하자. 그러면,

$$P_{\theta}(N) = \int_N\, dP_{\theta} = \int 1_N(x) \,Q_t(x)\, dG_{\theta}(t) = \int Q_t(N)\, dG_{\theta}(t) =0$$

임에 따라 $Q\_t(N) = 0$ (a.e. $G_{\theta}$)이다. $\tilde{N} = \\{t: Q_t(N) >0\\}$으로 두면 $G_{\theta}(\tilde{N}) = 0$임을 알 수 있다. 모든 $\theta$에 대하여 $P\_{\theta}(T\in\tilde{N}) = G_{\theta}(\tilde{N}) =0$가 성립하고, $\mu$는 model $\mathcal{P}$와 equivalent하기 때문에 $G^\*(N) = P^\*(T\in\tilde{N}) = 0$이다. 이는 다른 말로 $Q_t(N) =0 (a.e. $G^*$)를 의미한다. 따라서

$$\hat{P}(N) = \int Q_t(N)\, dG^*(t) = 0$$

임을 알 수 있다. $\hat{P}\ll \mu$이므로 density가 존재할 것이고, 이를 $h = d\hat{P}/d\mu$라고 두자. 그러면 궁극적으로

$$\frac{dP_{\theta}}{d\mu} = \frac{dP_{\theta}}{d\hat{P}}\frac{d\hat{P}}{d\mu} = g_{\theta}(T(\cdot))\cdot h$$

가 성립한다. 이로써 증명을 모두 마쳤다.

<br>
<br>
<br>

---
다음 post에서는 잠깐 Bayesian에서 등장하는 estimator에 대해 다룬다. 이는 estimation을 할 때 새로운 시각을 주는데, 특히 여러 estimator들의 risk를 비교하는 방법에 대한 또다른 방향성을 제시해줄 것이다.