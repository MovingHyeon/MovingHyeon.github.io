---
title: Normal Model, Noninformative Prior
date: 2025-01-23 16:07:40 +09:00
categories:
  - Statistics
  - Bayesian Analysis
math: true
pin: true
tags:
  - normal-model
  - weakly-informative
  - Jeffreys-invariance
  - noninformative
  - inverse-chi-square
---

이번 포스트에선 binomial model과 더불어 기본적으로 다루어지는 normal model을 정립해 볼 것이다. 이 모델들은 그 자체로는 복잡한 사회문제를 해결하기엔 너무 이상적이거나 간단하겠지만, 나중에 융합적인 모델로 확장해나갈 때 유용한 밑거름이 될 수 있다. 이와 더불어 정보가 충분치 않을 때 prior distribution을 설정하는 차선택들을 여럿 살펴보도록 하겠다.


<br>
<br>

### 3.1. Normal Model with Known Variance

먼저 variance $\sigma^2$은 알려져 있을 때 parameter $\mu$를 추론하는 베이즈 분석의 틀을 살펴보자. 데이터 하나에 대한 sampling distribution은 다음과 같을 것이다.

$$p(y\vert\theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{1}{2\sigma^2}(y-\theta)^2\right]$$

이를 likelihood의 시각, 즉 $\theta$에 대한 함수로 보면 이차다항식을 지수로 갖는 exponential function의 형태이다. Conjugate prior로써 적절한 것은

$$p(\theta) = e^{A\theta^2 +B\theta +C}$$

이때 $C$는 그저 상수로써 우리에게 중요한 상수는 $A, B$가 되겠다. 이들을 계산이 편리하도록 다음과 같이 매개화해보자.

$$ p(\theta)\propto \exp\left[-\frac{1}{2\tau_0^2}(\theta-\mu_0)^2\right]$$

즉, hyperparameter는 $(\mu_0, \tau_0^2)$으로 설정된 것이며, 함수 꼴을 보니 $\theta \sim N(\mu_0, \tau_0^2)$으로 둔 것임을 알 수 있다. 여기서도 hyperparameter는 별다른 무작위성 없이 이미 고정된 값으로 알려져 있다고 가정할 것이다. 이로써 우리는 다음의 posterior distribution을 얻을 수 있다.

$$\begin{align*}
p(\theta\vert y)&\propto \exp\left[-\frac{1}{2}\left(\frac{(y-\theta)^2}{\sigma^2}+\frac{(\theta-\mu_0^2)}{\tau_0^2}\right)\right]\\
&\propto \exp\left[-\frac{1}{2\tau_1^2}(\theta-\mu_1)^2\right]
\end{align*}$$

즉, $\theta\vert y \sim N(\mu_1, \tau_1^2)$이며, 이때

$$
\mu_1 = \frac{\frac{\mu_0}{\tau_0^2}+\frac{y}{\sigma^2}}{\frac{1}{\tau_0^2}+\frac{1}{\sigma^2}} \quad\text{and}\quad
\frac{1}{\tau_1^2} = \frac{1}{\tau_0^2}+\frac{1}{\sigma^2}
$$

두 번째 관계식에서 미루어보아 유독 이 모델에서 variance의 역수가 중요한 성질을 가지고 있는 듯하다. Variance의 역수를 *precision*이라고 하는데, normal distribution model에서는 이 precision의 역할이 매우 중요해진다. 여기에서는 prior의 precision과 data의 precision의 합이 곧 posterior precision이 됨을 보여준다. 

극단적인 상황을 살펴보자. 만약 prior의 영향이 너무 강력하여 $\mu_1=\mu_0$가 되기 위해서는 데이터 자체가 $y=\mu_0$가 되거나 prior의 precision이 무한대로 형성되어 $\tau_0^2=0$인 경우가 있다. 반대로 데이터의 영향이 너무 강력하여 $\mu_1 = y$를 초래하기 위해서는 prior 자체가 $\mu_0=y$가 되거나 data의 precision이 무한대로 형성되어 $\sigma^2=0$이 되어야 한다. 여기서도 어떻게 prior과 data가 그 정보를 잘 더해 posterior을 유기적으로 만들어내는지 극단적인 상황을 통해 살펴보았다.

이제 posterior prediction을 시도해보자. $\tilde{y}\vert y$의 분포는 다음과 같이 직접적으로 계산될 수 있다.

$$\begin{align*}
p(\tilde{y}\vert y) &= \int p(\tilde{y}\vert\theta)p(\theta\vert y)\, d\theta \\
&\propto \int\exp\left[-\frac{1}{2\sigma^2}(\tilde{y}-\theta)^2\right]\exp\left[-\frac{1}{2\tau_1^2}(\theta-\mu_1)^2\right]\, d\theta 
\end{align*}
$$

이때 우리는 데이터를 iid하게 추출하였기 때문에 $\theta$가 고정된 상태에서는 데이터끼리 독립, 그러므로 $p(\tilde{y}\vert\theta, y)=p(\tilde{y}\vert \theta)$이고, 이를 첫 줄에서 생략하여 명시했음을 참고하라. 이 적분식을 풀어도 우리가 원하는 distribution을 얻을 수 있겠지만 매우 복잡할 것이다. 적분 안에 있는 식은 결국 $(\tilde{y}, \theta)$의 joint distribution으로써 이차다항식을 지수로 갖는 exponential family이다. 그러므로 이를 통해 얻은 $\tilde{y}$의 marginal distribution 또한 같은 꼴일 것으로 예상할 수 있고, 이는 즉 $\tilde{y}$가 normal distribution을 따른다는 것을 의미한다. Normal distribution은 그 평균과 분산만 결정되면 그 분포 자체가 온전히 결정되므로 우리는 그 두 수치만 알면 된다. 새로운 데이터 $\tilde{y}$ 역시 동일한 sampling distribution을 따르므로 $E[\tilde{y}\vert\theta]=\theta$와 $Var(\tilde{y}\vert\theta)=\sigma^2$임을 참고하라. 이를 이용하면 우리는 다음을 얻는다.

$$\begin{align*}
E[\tilde{y}\vert y] &= E[E[\tilde{y}\vert\theta, y]\vert y] = E[E[\tilde{y}\vert\theta]\vert y]=E[\theta\vert y]=\mu_1 \\
& \\
Var(\tilde{y}\vert y)&= E[Var(\tilde{y}\vert\theta, y)\vert y] + Var(E[\tilde{y}\vert\theta, y]\vert y) \\
&= E[Var(\tilde{y}\vert \theta)\vert y] +Var(E[\tilde{y}\vert\theta]\vert y) \\
&= E[\sigma^2\vert y] + Var(\theta\vert y) \\
&=\sigma^2 + \tau_1^2 
\end{align*}
$$

이를 조금 해석해보면, posterior predictive distribution $\tilde{y}$는 posterior mean과 동일한 평균을 가지고 있으며, variance는 posterior가 가진 variance에 더하여 예측하는 과정 자체의 오차에 관한 variance가 더해져 있다. 

지금까지는 데이터 하나만을 추출한 것을 바탕으로 likelihood를 정립하여 모델을 구성했는데, 만약 $n$개의 exchangeable한 데이터를 추출한 경우 모델이 어떻게 변하는지 간단하게 살펴보자. 물론 여기서 크게 변하는 것은 딱히 없고 likelihood가 변하여 posterior distribution이 다음과 같이 변한다는 점만 유의하면 되겠다.

$$\begin{align*}
p(\theta\vert y) &\propto p(\theta)p(y\vert\theta) \\
&=p(\theta)\prod_{i=1}^n p(y_i\vert\theta) \\
&\propto \exp\left[-\frac{1}{2}\left(\frac{1}{\tau_0^2}(\theta-\mu_0)^2+\frac{1}{\sigma^2}\sum_{i=1}^n(y_i-\theta)^2\right)\right]
\end{align*}
$$

여기서 $y$는 이제 단일 데이터가 아닌 데이터 sequence $y=(y_1, \cdots, y_n)$으로 볼 수 있을 것이다. 만약 $\theta\vert y\sim N(\mu_n, \tau_n^2)$으로 둔다면 우리는 다음의 관계식을 얻을 수 있다.

$$\mu_n = \frac{\frac{\mu_0}{\tau_0^2}+\frac{n\bar{y}}{\sigma^2}}{\frac{1}{\tau_0^2}+\frac{n}{\sigma^2}}\quad\text{and}\quad \frac{1}{\tau_n^2}=\frac{1}{\tau_0^2}+\frac{n}{\sigma^2}
$$

그러므로 단일 데이터로만 모델을 구성했을 때와 그렇게 큰 차이가 없음을 알 수 있다. 다만, 데이터가 여러 개였으므로 이를 가장 잘 요약해주는 sufficient statistic $\bar{y}$에 의해 표현된 것이 눈에 띌 텐데, precision의 경우도 prior precision과 $\bar{y}$의 precision, 이 둘의 합으로 posterior precision이 완성되었음이 보일 것이다.

<br>
<br>

### 3.2. Normal Model with Known Mean

Sample distribution이 위와 같이 똑같은 normal distribution이면서 이번에는 $\theta$는 알고 있고 $\sigma^2$에 대해선 베이즈 추론을 하는 모델을 설계해보자.  $n$개의 데이터 sequence $y=(y_1, y_2, \cdots, y_n)$에 대하여 이들이 모두 iid하게 추출되었다고 가정하자. 그러면 likelihood는 다음과 같다.

$$\begin{align*}
p(y\vert \sigma^2) &\propto \sigma^{-n}\exp\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\theta)^2\right] \\
&= (\sigma^2)^{-n/2}\exp\left[-\frac{n}{2\sigma^2}v\right]
\end{align*}
$$

이번 경우에는 $\sigma^2$에 대한 함수이므로 이의 형태에 맞게 식을 간편화했다. $y=\frac{1}{n}\sum_{i=1}^n (y_i-\theta)^2$은 sufficient statistic일 것이다. 이 꼴과 걸맞는 conjugate prior로는 다음을 제시할 수 있다.

$$p(\sigma^2)\propto (\sigma^2)^{-\alpha+1} e^{-\beta/\sigma^2}$$

이 형태를 가지는 distribution을 *scaled inverse-chi distribution*이라고 한다. 만약 $X\sim \chi_{\nu}^2$일 때, $1/X \sim \text{Inv-}\chi^2(\nu)$인 셈이고, 이때 $\text{Inv-}\chi^2$는 inverse-chi distribution을 의미한다. 그리고 어떤 상수배를 취하여 scaled한 것, $\nu\sigma^2/X\sim \text{Inv-}\chi^2(\nu, \sigma^2)$을 정의할 것이다. 이를 모두 정리하면 다음 표와 같다.


| Distribution              | Notation                                                                                                                     | Parameters                                | Density                                                                                                                                                                                            |
| ------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Gamma                     | $$\theta\sim\text{Gamma}(\alpha, \beta)$$                                                                                    | shape $\alpha>0$, inverse-scale $\beta>0$ | $$\begin{align*}p(\theta)&=\frac{\beta^{\alpha}}{\Gamma(\alpha)}\theta^{\alpha-1}e^{-\beta\theta}, \quad \theta>0 \\&\propto \theta^{\alpha-1}e^{-\beta\theta}\end{align*}$$                       |
| Inverse-Gamma             | $$\theta\sim\text{Inv-gamma}(\alpha, \beta)$$                                                                                | shape $\alpha>0$, scale $\beta>0$         | $$\begin{align*}p(\theta)&=\frac{\beta^{\alpha}}{\Gamma(\alpha)}\theta^{-(\alpha+1)}e^{-\beta/\theta}, \quad\theta>0 \\ &\propto \theta^{-(\alpha+1)}e^{-\beta/\theta}\end{align*}$$               |
| Chi-square                | $$\begin{align*}\theta&\sim\chi^2_{\nu}\\&\equiv\text{Gamma}(\alpha=\nu/2, \beta=1/2)\end{align*}$$                          | deg. of freedom $\nu>0$                   | $$\begin{align*}p(\theta)&=\frac{2^{-\nu/2}}{\Gamma(\nu/2)}\theta^{\nu/2-1}e^{-\theta/2}, \quad\theta>0\\ &\propto \theta^{\nu/2-1}e^{-\theta/2}\end{align*}$$                                     |
| Inverse-chi-square        | $$\begin{align*}\theta&\sim\text{Inv-}\chi^2_{\nu}\\&\equiv\text{Inv-Gamma}(\alpha=\nu/2, \beta=1/2)\end{align*}$$           | deg. of freedom $\nu>0$                   | $$\begin{align*}p(\theta)&=\frac{2^{-\nu/2}}{\Gamma(\nu/2)}\theta^{-(\nu/2+1)}e^{-1/(2\theta)}, \quad\theta>0 \\&\propto \theta^{-(\nu/2+1)}e^{-1/(2\theta)}\end{align*}$$                         |
| Scaled Inverse-chi-square | $$\begin{align*}\theta&\sim\text{Inv-}\chi^2(\nu, s^2)\\&\equiv\text{Inv-gamma}(\alpha=\nu/2, \beta=\nu s^2/2)\end{align*}$$ | deg. of freedom $\nu>0$, scale $s>0$      | $$\begin{align*}p(\theta)&=\frac{(\nu/2)^{\nu/2}s^{\nu}}{\Gamma(\nu/2)}\theta^{-(\nu/2+1)}e^{-\nu s^2/(2\theta)}, \quad\theta>0 \\ &\propto\theta^{-(\nu/2+1)}e^{-\nu s^2/(2\theta)}\end{align*}$$ |


위 distribution들의 정확한 density를 숙지할 필요는 없지만, 계수는 무시하더라도 함수 꼴이 어떻게 생겼는지를 알아두면 굉장히 편리하다.

다시 돌아와서, scaled inverse-chi-square density의 꼴을 보았을 때 우리가 찾던 conjugate prior의 꼴임을 알 수 있다. 그러므로 $\sigma^2\sim \text{Inv-}\chi^2(\nu_0, \sigma_0^2)$으로 두면 다음의 posterior density를 얻는다.

$$ \begin{align*}
p(\sigma^2\vert y)&\propto p(\sigma^2)p(y\vert\sigma^2) \\
&= (\sigma^2)^{-[\nu_0/2+1]}\exp\left[-\frac{\nu_0\sigma_0^2}{2\sigma^2}\right]\cdot (\sigma^2)^{-n/2}\exp\left[-\frac{n}{2\sigma^2}\nu\right] \\
&= (\sigma^2)^{-[(\nu_0+n)/2+1]}\exp\left[-\frac{\nu_0\sigma_0^2+n\nu}{2\sigma^2}\right]
\end{align*}
$$

즉,

$$\sigma^2\vert y \sim \text{Inv-}\chi^2\left(\nu_0+n, \frac{\nu_0\sigma_0^2+n\nu}{\nu_0+n}\right)$$

사후 자유도(degrees of freedom)가 prior distribution의 자유도에 데이터의 개수를 합한 값으로 도출되었다. 또한, scale 계수 또한 prior distribution에서의 scale 계수에 데이터의 scale, 즉 데이터 분산의 가중합임을 알 수 있다. 여기서 prior distribution을 위와 같이 설정함으로써 마치 평균적으로 $\sigma_0^2$의 분산을 가지는 데이터 $\nu_0$개가 가지는 정보를 제공해주는 것으로 더불어 해석할 수 있다.



<br>
<br>

### 3.3. Poisson Model

만약 데이터를 적합하기에 Poisson distribution이 적절한 경우 베이즈 모델을 설계해보도록 하자. Poisson distribution은 binomial distribution과 마찬가지로 어떤 사건이 발생하는 횟수를 셀 때 자주 활용하는데, 주로 발생할 확률이 희박한 사건에 많이 쓰인다. 대표적인 분야로 질병에 관한 역학조사(epidemiology)를 다루는 학문과 관련이 깊다.

만약 사건이 $\theta$의 빈도(rate)로 발생하고, 그러한 사건이 발생한 횟수를 담은 데이터 $n$개 $y=(y_1, \cdots, y_n)$가 독립적으로 측정된다면 이에 대응되는 likelihood는 다음과 같다.

$$ \begin{align*}
p(y\vert\theta) &= \prod_{i=1}^n \frac{1}{y_i!}\theta^{y_i}e^{-\theta} \\
&\propto \theta^{T(y)}e^{-n\theta} \\
&= \exp\left[T(y)\log\theta-n\theta\right]
\end{align*}$$

이때 $T(y)=\sum_{i=1}^n y_i$는 sufficient statistic, 그리고 $\eta(\theta)=\log\theta$이 natural parameter임을 알 수 있다. 만약 prior distribution을 conjugate로 설정하고 싶다면 이는 다음과 같은 꼴을 취하고 있어야 할 것이다.

$$p(\theta) \propto \theta^{\nu}e^{-\eta\theta}$$

이와 같은 꼴의 density를 가진 distribution으로는 위의 표를 참고하면 gamma distribution이 있다는 것을 알 수 있다. 그러므로 $\theta\sim \text{Gamma}(\alpha, \beta)$로 설정하면 posterior distribution은 다음과 같이 도출됨을 알 수 있다.

$$\theta\vert y\sim \text{Gamma}(\alpha+n\bar{y}, \beta+n)$$

번외로, 데이터 $y$의 marginal distribution, 즉 prior predictive distribution을 계산해보자.

$$\begin{align*}
p(y) &= \frac{\text{Pois}(y\vert\theta)\text{Gamma}(\theta\vert\alpha, \beta)}{\text{Gamma}(\theta\vert\alpha+y, 1+\beta)} \\
&=\frac{\Gamma(\alpha+y)\beta^{\alpha}}{\Gamma(\alpha)y!(1+\beta)^{\alpha+y}} \\
&= {\alpha+y-1\choose y}\left(\frac{\beta}{\beta+1}\right)^{\alpha}\left(\frac{1}{\beta+1}\right)^y
\end{align*}$$
이 함수는 *negative binomial distribution*의 density로 알려져 있다. 즉,

$$y\sim \text{Neg-bin}(\alpha, \beta)$$

우리는 여기서 negative binomial distribution이 'gamma distribution을 기반으로 하는 Poisson distribution의 *mixture distribution*임을 알 수 있다. 이는 추후에 Poisson distribution의 robust한 대체 분포로 negative binomial distribution을 활용하게 될 때 다시금 다룰 것이다.


비슷하게, 만약 데이터가 exponential distribution을 따른다면 베이즈 모델이 어떻게 정립되는지 살펴보자. Exponential distribution은 생존 분석 등에서 일종의 '대기 시간'에 대한 분석이나, 다른 시간 개념으로 측정되는 양수 데이터에 많이 활용한다. Sampling distribution은 다음과 같이 주어진다.

$$ p(y\vert \theta)=\theta\exp[-y\theta], \quad y>0$$

이때 $\theta$는 사건이 발생하는 rate라고 볼 수 있다. 한편, exponential distribution은 수학적으로 gamma distribution의 특별한 경우로 볼 수 있는데, $\text{Exp}(\theta)\equiv\text{Gamma}(1, \theta)$를 만족한다. 아무쪼록 이를 likelihood, 즉 $\theta$의 함수로 보았을 때 알맞는 conjugate prior distribution 또한 gamma distribution임을 알 수 있다. $\theta\sim \text{Gamma}(\alpha, \beta)$로 두게 되면 posterior distribution으로 $y\vert\theta\sim\text{Gamma}(\alpha+1, \beta+y)$를 얻을 수 있다. 

지금까지 다룬 각종 sampling distribution에 따른 conjugate는 다음과 같다

| Conjugate Family    | Sampling Distribution    |
| ------------------- | ------------------------ |
| Beta                | Binomial                 |
| Normal              | Normal, $\sigma^2$ known |
| Scaled Inv-$\chi^2$ | Normal, $\mu$ known      |
| Gamma               | Poisson                  |
| Gamma               | Exponential              |

<br>
<br>

### 3.4. Noninformative Prior Distributions

만약 적절하게 주어진 사전정보가 많지 않을 때, 사전 정보를 바탕으로 만들어져야 하는 prior distribution을 구성하기 쉽지 않을 수 있다. 또한, 이러한 경우 prior distribution을 임의로 구성하다가 자칫 거짓된 자작 정보를 포함할 수 있게 된다. 한창 베이즈 이론이 정립되고 있을 당시, 사전 정보가 부재일 때 prior distribution이 최소한의 역할만을 취하는 것이 *보장된* 함수의 형태에 대해 오랜 고민이 있었다. 이와 같은 prior distribution은 굉장히 애매모호하고, 어느 한 곳에 집중되지 않고 평평하고, 넓게 분포하여 퍼져있는 형태를 취하게 된다. 이들은 모두 prior distribution에게는 정보가 없다(*noninformative*)는 것을 표현하기 위해 갖게 된 성질이며, 그러므로 이러한 prior을 사용하는 것은 *오로지 측정된 데이터가 그 모든 것을 설명해야 한다*(*to let the data speak themselves*)는 근거에 기반했다. 이를 사용한다는 것은 이후 추론 과정에서도 오로지 외부 데이터에 의존한 결론이 도출될 것이고, prior distribution은 추론 과정에 그 어떤 영향도 취하지 않는다는 것을 의미했다.

<br>
#### 3.4.1. To Let the Data Speak Themselves

이를테면, normal model에서 $\sigma^2$이 알려진 경우를 다시 살펴보자. 우리는 위에서 conjugate prior으로 $\theta\sim N(\mu_0, \tau_0^2)$를 설정했다. 만약 prior precision $1/\tau_0^2$이 data precision $n/\sigma^2$에 비해 충분히 작은 상황을 생각하자. 즉, prior distribution의 영향력은 거의 없고 이후의 추론은 오로지 데이터에 달린 상황이다. 이 상황에서의posterior distribution은 $\tau_0^2=\infty$를 대입할 때와 같을 것이며, 이는 다음과 같다.

$$ p(\theta\vert y)\approx N(\theta\vert \overline{y}, \sigma^2/n)$$

이를 다시 거꾸로 생각해보자. 위와 같은 posterior distribution이 나오기 위해서는 prior distribution이 $\theta\in(-\infty, \infty)$에서 $p(\theta)\propto 1$을 만족하면 됨을 알 수 있다. 이를 조금 해석해보면 그 어떤 $\theta$에 대해서도 불확실성에 대한 정보가 존재하지 않아 고르게 설정했다고 볼 수 있다. 하지만 이와 같은 density는 그 넓이가 무한하게 되므로 수학적으로 불가능한 형태이다. 이와 같이, 이론적으로 불가능한 density의 꼴을 갖게 되는 것을 **improper**하다고 표현한다. 방금 제시한 prior distribution도 일종의 improper 함수이다. 우리가 눈여겨 볼 것은 지금 이 상황도 그러하듯, 비록 prior distribution은 improper하지만 posterior distribution은 충분히 proper할 수 있다. 

다른 예시로, normal model에서 $\mu$가 알려진 상황을 살펴보자. Conjugate prior distribution으로 $\theta\sim \text{Inv-}\chi^2(\nu_0, \sigma_0^2)$으로 설정한 것을 기억할 것이다. 만약 prior degrees of freedom $\nu_0$가 데이터의 degrees of freedom $n$에 비해 매우 작다면, posterior distribution은 $\nu_0=0$일 때로 근사하여 다음과 같이 될 것이다.

$$ p(\sigma^2\vert y)\approx \text{Inv-}\chi^2(\sigma^2\vert n, v)$$

비슷하게 거꾸로 생각해보면, 이와 같은 posterior distribution이 도출되기 위해서는 prior distribution을 $p(\sigma^2)=\text{Inv-}\chi^2(\sigma^2\vert 0, 0) \propto 1/\sigma^2$으로 설정해야 한다. 하지만 $\sigma\in(0, \infty)$ 구간에서 해당 density는 적분하였을 때 $+\infty$이므로 역시 improper함을 알 수 있다. 그러므로 이 상황 또한 prior은 improper하지만 posterior은 proper인 또다른 예가 되겠다.

그렇다고 하여 prior가 improper라고 해도 posterior가 항상 proper하게 도출되는 것은 아니다. 위의 예시들은 그저 prior가 improper일지라도 posterior은 density의 수학적 정의를 충분이 충족할 수 있음을 보여주고 싶었을 뿐이다. 비록 prior density는 수학적으로 정의되진 않지만, 이 일련의 과정은 우리가 *noninformative* prior distribution으로 요구되었던 조건, 데이터(그러므로 likelihood)가 prior를 지배하여 오로지 데이터가 사후 추론에 영향을 주도록 하는 것을 정확히 만족시켜준다. 그러므로 모델 전체적인 관점에서 이러한 prior distribution의 설정은 의미가 있다고 볼 수 있다. 항상 유의할 점은 그러한 철학에 근거하여 설정한 noninformative prior distribution이 posterior distirbution까지 improper로 만들어버린다면, 이는 더이상의 추론을 할 수 없으므로 의미가 없어질 것이다.

<br>

#### 3.4.2. Jeffreys' Invariance Principle

이전 포스트에서 소개한 Laplace의 여아 출산률 분석을 위해 활용했던 Bayes model를 다시 살펴보자. 그때 활용했던 prior distribution은 여아 출산률 $p$에 대한 uniform이었는데, Laplace는 간접적으로 이 prior를 사용한 이유로 '정보의 부족'을 제시했었다. 다시 말해, Laplace는 얼떨결에 우리가 현재 말하는 noninformative prior distribution을 선제적으로 활용한 것이다. 하지만 여기엔 사실 다수의 베이지안이 해결치 못한 문제가 하나 있었다.

여아 출산률의 log-odds ratio $\eta=\log\frac{p}{1-p}$을 새로운 변수로 채택하자. Laplace의 주장에 따르면 우리는 불확실성에 대한 우위를 결정할 사전 정보가 없으므로 $\eta$에 대해서도 noninformative prior distribution을 지정해야 한다. 즉, $f_{\eta}(\eta)\propto 1$인 셈이다. 

여기서 한 가지 문제점이 발생하게 되는데, 이미 정당한 논리대로 설정한 $f_p(p)\propto 1$과 수학적으로 충돌이 발생한다는 점이었다. 만약 $f(p)\propto 1$을 가정한 후 변수변환법을 활용하여 $\eta$의 density를 구하면

$$\hat{f}_{\eta}(\eta) = f_p(p(\eta))\left\vert \frac{dp}{d\eta}\right\vert = f_p\left(\frac{1}{1+e^{-\eta}}\right)\frac{e^\eta}{(1+e^{\eta})^2}=\frac{e^{\eta}}{(1+e^{\eta})^2}$$

즉, 우리는 $\eta$에 대한 아무런 사전 정보가 없었지만, 균등한 불확실성을 가지지 않는다. 비록 $\eta$가 $p$의 monotone transform(그러므로 일대일로 연결되는) 이었음에도 불구하고, 하나의 변수에 대해 불확실성의 선호도를 평평하게 만들면 변수 변환을 취한 변수에 대해서 그 불확실성의 선호도가 평평하게 유지되지 않을 수 있다. 이 현상은 Jeffreys가 나타나기 전까지는 베이지안 통계에서 매우 큰 비판점으로 남겨졌다.

Jeffreys가 이 논의를 시작하기에 앞서 고민했던 점은, 과연 어떤 변수 변환을 취해도 보편적으로 noninformative prior로 받아들일 수 있는 density function이 있을까였다. 물론 Laplace의 uniform prior은 완전히 틀린 주장은 아니었던 것이, 만약 parameter space가 discrete(이산적)하고 finite했으면, 그 어떤 변수 변환 아래에서도 uniform distribution은 유지되었을 것이었다. 진짜 문제는 parameter space가 continuum인 경우가 대부분이고, 그런 경우에서는 uniform distribution이 일반적인 변수 변환 아래에서는 유지되지 않는다는 것이었다. 그러므로 Jeffreys가 주장하기를, 받아들일 수 있는 noninformative prior distribution은 parameter의 그 어떤 monotone transformation아래에서도 그가 말하는 *principle*(즉, 그가 주장하는 noninformative prior를 만드는 방법)로 만들어진 prior가 invariant(불변)해야 한다.

만약 우리가 처음에 분석하는 parameter $\theta$에 대해, 그가 제시한 principle로 만들어진 noninformative prior를 $p_{\theta}(\theta)$라고 두자. 이제 충분히 smooth한 monotone transformation $h$에 대하여 변환된 parameter $\eta=h(\theta)$를 살펴보자. Jeffreys의 principle로 만들어진 noninformative prior를 $p_{\eta}(\eta)$라고 두고, $p_{\theta}(\theta)$에서 변수 변환을 통해 얻은 $\eta$의 distribution을 $\tilde{p}_{\eta}(\eta)$라고 하자. 그의 invariance 법칙에 따르면 두 distribution은 동일, 즉 $p_{\eta}(\eta)=\tilde{p}_{\eta}(\eta)$여야 한다. 변수변환 공식을 적용하면 성립해야 하는 식은 결국 다음과 같다.

$$ p_{\eta}(\eta)=p_{\theta}(\theta)\left\vert\frac{d\theta}{d\eta}\right\vert=p_{\theta}(\theta)\cdot\frac{1}{\vert h'(\theta)\vert} $$

한편, Jeffreys는 위의 식이 성립하는 실질적인 density를 제시했다. 그는 Fisher information을 기반으로 함수의 꼴을 제시했다. Fisher information에서 변수변환을 활용하면 우리는 다음을 얻을 수 있다. $\theta$를 기반으로 주어지는 model을 $X\sim p_{\theta}(x\vert\theta)$, $\eta$를 기반으로 주어지는 model를 $X\sim p_{\eta}(x\vert\eta)$로 두자. 또한, 각각의 모델에 주어지는 Fisher information을 $I_{\theta}$와 $I_{\eta}$라고 하면,

$$\begin{align*}
I_{\theta}(\theta)&=E\left[\left.\left(\frac{\partial\log p_{\theta}(x\vert\theta)}{\partial\theta}\right)^2\right\vert\theta\right] \\
&=E\left[\left.\left(\frac{\partial\log p_{\eta}(x\vert h(\theta))}{\partial\theta}\right)^2\right\vert h(\theta)\right]\\
&= E\left[\left.\left(\frac{\partial\log p_{\eta}(x\vert\eta)}{\partial\eta} h'(\theta)\right)^2\right\vert\eta\right]\\
&=I_{\eta}(\eta)[h'(\theta)]^2
\end{align*}
$$

가 성립한다. 복잡해보이지만 사실 chain rule 한 번 쓴 게 전부이다. 여기서 주목할 점은 위로부터 우리는 다음의 원하는 식을 얻을 수 있다는 것이다. 

$$ I_{\eta}(\eta)^{1/2} = I_{\theta}(\theta)^{1/2}\cdot \frac{1}{\vert h'(\theta)\vert}$$

그러므로 noninformative prior distribution으로 

$$ p(\theta) \propto I_{\theta}(\theta)^{1/2}$$

로 두는 것이 우리가 원하는 principle이 된다. 그리고 이것이 Jeffreys가 제시한 noninformative prior distribution이었다. 이 아이디어는 multivariate case에 대해서도 확장이 충분히 가능하지만 조금의 논의가 더 필요로 한다.

Binomial model에서 Jeffreys의 noninformative prior distribution을 구해보자. 측정된 데이터 $y\sim B(n, \theta)$에 대해서 먼저 log-likelihood는 다음과 같다.

$$\log p(y\vert\theta) = y\log\theta + (n-y)\log(1-\theta) + c$$

그러므로 Fisher information은 다음과 같다.

$$ I(\theta) = -E\left[\left. \frac{\partial^2\log p(y\vert\theta)}{\partial\theta^2}\right\vert \theta\right]=\frac{n}{\theta(1-\theta)}$$

따라서 Jeffreys' prior distribution은 $p(\theta)\propto \theta^{-1/2}(1-\theta)^{-1/2}$이다. 이는 $\text{Beta}(1/2, 1/2)$의 density임을 알 수 있다.

이전에 uniform distribution으로 prior를 택한 경우에서는  $p(\theta)\propto 1$, 즉 정의되진 않은 분포이지만 형식적으로 표현하면 $\text{Beta}(1, 1)$이다. 또, natural parameter $\text{logit}(\theta)=\log\frac{\theta}{1-\theta}$에 대해서 uniform distribution을 prior로 택한 경우에서는 $p(\text{logit}(\theta))\propto 1$, 혹은 $p(\theta)\propto\theta^{-1}(1-\theta)^{-1}$이므로 $\text{Beta}(0, 0)$을 따른다. 이렇듯 방법에 따라, 혹은 철학적 근간에 따라 prior distribution은 다르게 잡힐 수 있다.

<br>

#### 3.4.3. Pivotal Quantities

위의 binomial model의 예시 같이 noninformative prior distrbution은 그 방법에 따라 다른 형태의 density를 가졌다. 하지만 location parameter와 scale parameter의 경우, 그 어떤 principle을 가져와도 결국 그 형태는 하나로 정해진다.

먼저 location parameter $\theta$에 대해 살펴보자. 분포의 *위치*를 정해준다는 의미에서 붙여진 이 parameter는 똑같이 생긴 density function이 $\theta$에 따라 중심 위치를 결정한다. 그러므로 이 분포의 density는 $p(y-\theta\vert\theta)$ 꼴을 가지며, $y$와 $\theta$와는 무관하게 $u=y-\theta$로만 결정되므로 $f(u)$라고 두자. 즉, $u=y-\theta$는 *pivotal quantity*라고 할 수 있다. 만약 noninformative prior로 설정이 된 경우, posterior distribution $p(y-\theta\vert y)$도 분명히 $f(u)$를 갖게 될 것이다. 따라서 Bayes' rule $p(y-\theta\vert y)\propto p(\theta)p(y-\theta\vert\theta)$에 따라 noninformative prior density는 $p(\theta)\propto 1$로 결정됨을 알 수 있다.

이제 $\theta$가 scale parameter인 경우를 살펴보자. $y$의 density가 $p(\frac{y}{\theta}\vert\theta)$의 꼴을 가지며, $y$와 $\theta$와는 무관하게 $u=\frac{y}{\theta}$에 대한 함수라고 가정하자. 그러면 $u=\frac{y}{\theta}$는 여기서 pivotal quantitiy가 된다. 만약 $\theta$에 대하여 noninformative prior distribution을 설정했다면, posterior distribution $p(\frac{y}{\theta}\vert y)$도 분명히 $g(u)$를 갖게 될 것이다. 변수변화에 의하여 

$$\begin{align*}
p(y\vert\theta) &= \frac{1}{\theta}p(u\vert\theta)\\
p(\theta\vert y) &= \frac{y}{\theta^2}p(u\vert y)
\end{align*}$$

가 성립한다. Bayes' rule $p(\theta\vert y)\propto p(\theta)p(y\vert\theta)$에 따르면 prior distribution은 $p(\theta)\propto \theta^{-1}$가 된다.

이렇게 pivot을 중심으로 noninformative prior distribution을 결정하는 건 생각보다 자주 쓰이는 방법이다. 위에서 다룬 것보다 더 복잡한 상황에서도 비슷한 접근 방식으로 진행할 수 있다. 하지만, 위 방법도 때로는 posterior distribution이 improper가 될 여지가 있다는 문제점이 있곤 하다. 이 실례에 대해서는 hierarchical model를 다룰 때 등장할 것이다.

<br>

<br>

### 3.5. Difficulties with Noninformative Prior Distributions

이렇게 noninformative prior distribution을 결정하는 방법으로 여러 가지를 살펴보았지만, 그럼에도 몇 가지 문제를 피할 수 없다.

먼저, 베이즈 분석의 가장 큰 축에 해당하는 문제이지만 애초에 prior distribution을 설정하는 과정에 큰 맥락과 패턴이 있을 순 있어도 정확하게 정형화된 방법은 없다는 것이다. 만약 likelihood의 역할이 매우 큰 상황이라면, prior distribution으로 얼추 평평한 것을 골라야 한다는 맥락은 생기지만 어떤 구체적인 distribution을 선택해야 하는지는 여전히 판가름하기 어렵다. 혹은 오히려 하나의 결정으로 판가름하는 것이 부적절할 수도 있겠다.

많은 분석에서 발생하는 문제로, 어떤 parameter를 기준으로 prior distribution을 결정해야 할지에 대해서 명확하지 않다. 많은 경우 어떤 한 변수에 대해서 flat한 density를 가정한다고 한들, 그 parameter에 변수 변환을 취한다면 새로 매개화된 변수는 flat density를 가지지 않을 것이다. 이는 Laplace가 일컬은 *principle of insufficient reason*에서도 나아가 등장하는 큰 문제 거리인데, 결국 어느 정도의 scale를 취한 parameter에 noninformative prior distribution을 설정하는 principle를 적용해야 하는지 불명확하다. 

이후에도 improper prior distribution으로 설정한 것들로 모델을 비교할 때도 다른 문제들이 등장하는데, 이는 이후에 다루어보도록 하겠다.

<br>

<br>

### 3.6. Weakly Informative Prior Distributions

우리는 prior distribution을 설정할 때 사전 정보가 충분히 있음에도 density 자체는 proper하지만 사전 정보를 의도적으로 약하게 반영할 때도 있는데, 그렇게 결정된 prior distribution을 **weakly informative**하다고 한다. 대부분의 분석에서 우리는 정보에 대해 눈을 감아버리기보다는 현실적인 사회에서 존재하는 정보를 소량 획득하여 prior distribution에 적용, 그리하여 posterior distribution이 보다 더 의미가 부여될 수 있도록 모델을 정립한다. 이를 테면, 만약 여아 출생률 $p$를 추론하는 과정에서 남녀 성비가 크게 변동이 있어도 중앙에서 크게 벗어나지 않는다는 사회적 정보에 따라 prior distribution을$N(0.5, 0.1^2)$과 같이 $0.4$와 $0.6$ 사이에 몰려 있는 distribution으로 설정할 수 있을 것이다. 혹은 conjugacy로 제시할 수 있는 $\text{Beta}(20, 20)$ 또한 설득력 있는 선택인데, $0.4$와 $0.6$ 사이로 약 $80\%$의 확률이 차지하고 있기 때문이다. 

실질적 사회 분석 문제에서 데이터 분석가는 통계 모델에 곧바로 적용할 수 있는 것보다 더 많은 사회적 사전 정보를 주로 가지고 있게 된다. 이러한 현상은 likelihood를 설정할 때뿐만 아니라 prior distribution을 결정할 때도 문제가 발생하게 된다. 이러한 정보들을 다 활용하지 않은 이유로 늘 거론되는 것이 몇 가지 있다. 모델을 조금 더 간편하게 만들기 위해서, 사전 정보를 '확률'로 정확하게 표현하기 어렵기 때문에, 계산을 간편화하기 위해, 혹은 혹시 습득하지 않은 다른 허위 정보들도 함께 포함하는 것을 피하기 위함 등. 하지만 마지막 이유를 제하고는 사실 이러한 이들을 고려하여 모델을 정립하는 것은 편리성 등을 포기하고 모델을 매우 세밀하고 정확하게 만들었을 때와 그 결과에서 큰 차이를 보이진 않는다. 이 문제에 대해서, 즉 'likelihood와 prior distribution의 편리성'과 '모델의 정확성' 두 화두를 두고 어느 것에 무게를 실어야 할지에 대해서는 추후에 살펴볼 것이다.

Weakly informative prior distribution을 설정하는 방법으론 직관적으로 다음 두 자기를 생각할 수 있을 것이다.

1. Noninformative prior distribution으로 시작하여 조금씩 정보를 추가해나가며 추론이 합리적인 수준까지 가능토록 한다.
2. 굉장히 informative한 prior distribution을 마련하고, 자신이 생각하는 불확실성과 과거 연구에서 발굴되는 데이터에 기반한 새로운 prior distribution 모두를 고려하도록 조금 더 느슨하게 확장해나간다.

물론 두 방법 다 순수한 weakly informative prior distribution을 만들어내지 못할 수 있다. 첫 번째 방법의 경우, noninformative라고 알려져서 활용한 prior distribution이 사실 이미 너무 강력한 것일 수 있다. 이를 테면, 희귀 질병의 전염률을 분석하는 분석에서 weakly informative prior distribution을 만들어내기 위해 $\mathcal{U}(0, 1)$을 세팅했다고 하자. 정보력이 약한 데이터가 있는 존재하는 상황에서 prior distribution을 uniform으로 두면 오히려 전염률이 굉장히 과대예측된다. 적절한 weakly informative prior를 설정했다면 posterior distribution이 작은 숫자 범위에서만 집중될 것이다. 두 번째 경우, 강력한 information으로 설정했다고 생각한 prior distribution이 어떤 시각에서는 생각 외로 약했을 수도 있다. 물론 이 모든 일련의 설명이 posterior distribution이 설득력 있는 결과를 가져오지 못했을 때 무조건 prior distribution의 설정 오류로 바라보라는 것은 아니다. Posterior distribution의 결과가 어떻든 이를 바탕으로 설명해나가는 것이 우리의 몫일 것이다. 하지만, 굳이 사전 정보가 충분한 상황에서도 기본적으로 설계되어 있는 noninformative model을 곧이곧대로 적용할 필요는 없다. 

한편, 뻔히 사후 추론을 조금 더 향상시킬 수 있는 정보가 있음에도 이를 사용하지 못하는 상황이 있을 때도 있다. 즉, 수학적으로 정립된 constraints(제약), 예컨대 symmetry principle(어떤 기준점을 기준으로 클 수도 있고 작을 수도 있는 상황에서 정보가 충분치 않아 양방향을 동일한 불확실성으로 고려해야 한다는 법칙)을 깨고 뭔가 조금 그럴듯한 정보를 추가하면 더 의미 있는 결론을 도출할 수 있는 등의 경우를 일컫는다. 하지만 조심해야 하는 것은, 그런 관련한 정보들로 인하여 prior distribution을 너무 예단하는 방향으로 설정하게끔 냅두어서는 안 된다는 것이다. Symmetry principle 등같은 수학적 가정은 굉장히 정당한 논리 하에서 설정된 하나의 가설같은 것이므로, 이에 반하는 방향으로 분석을 이끌고 싶다면 이보다도 더 상당히 정당화되는 증거 혹은 증명과 함께 해야할 것이다. 

<br>

<br>

---
다음 포스트에서는 multiparameter, 즉 parameter가 multivariate(다차원)일 때의 대표적인 베이즈 모델을 정립해볼 것이다. Parameter가 많아지면, 그 중에서도 관심이 있는 parameter와 그저 수학적으로 잘 정립될 수 있도록 도와주는 역할의 parameter가 있는데, 이 둘을 구분하며 실질적으로 추론하고 싶은 parameter의 분석을 이끌어내기 위해서 어떠한 과정을 취하는지도 함께 알아보도록 한다.