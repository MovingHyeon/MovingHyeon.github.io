---
title: The Rao-Blackwell Theorem
date: 2024-04-03 15:37:00 +09:00
categories: [Statistics, Mathematical Statistics]
math: true
pin: true
tags:
  [
    probability,
    mathematical statistics,
    suffcient,
    sufficiency,
    Rao-Blackwell,
    convex,
    convex loss
  ]
---

# Convex Loss

우리는 많은 loss function 중에서도 convex loss<sub>볼록손실함수</sub>에 대해서 다루어볼 것이다. 이에 앞서, 먼저 convexity<sub>볼록성</sub>에 대해 수학적으로 간단하게 살펴보자.

<br>

> **Definition 9.1** Convex set $\mathcal{C}\subset\mathbb{R}^p$ 위에서 정의된 real-valued function<sub>실변수함수</sub> $f$가 *convex*<sub>볼록, 아래로 볼록</sub>하다는 것은, $\mathcal{C}$ 안의 모든 $x\ne y$와 $\gamma\in(0, 1)$에 대하여
>
> $$f(\gamma x + (1-\gamma)y) \le \gamma f(x) + (1-\gamma)f(y)$$
>
> 가 성립한다는 것이다. $f$가 *strictly convex*<sub>순볼록</sub>하다는 것은 위의 inequality가 equality 없이 성립한다는 것이다.
>

<br>

위의 식을 조금 기하학적으로 해석하면, 좌변은 $x$와 $y$ 사이를 내분하는 어떤 점에서의 함숫값이라고 볼 수 있고, 우변은 $(x, f(x))$와 $(y, f(y))$를 지나는 직선 위를 내분한 점의 높이라고 할 수 있다. 그러므로 convex function은 어느 두 점을 잡아도 그 사이에서 함수가 그 두 점을 이은 직선보다 낮게 위치하고 있는 개형을 띤다고 할 수 있다. 

조금 다른 관점에서 convexity를 다시 살펴보자. 다음 theorem은 1차원 상에서 convexity의 특징을 살펴본 것이지만, 이는 hyperplane theorem으로 일반화될 수 있다.

<br>

> **Theorem 9.2** $f$가 open interval $\mathcal{C}$ 위에서 convex한 함수하고 하고, $t$가 $\mathcal{C}$ 위의 임의의 점이라고 할 때, 각 $t$에 대하여 다음을 만족하는 constant $c$가 존재한다.
>
> $$f(t) + c(x - t) \le f(x), \quad \forall x\in\mathcal{C}$$
>
> 만약 $f$가 strictly convex하다면, 위에서 equality를 제외한 strict inequality가 성립하는 constant $c$가 존재한다.
>

<br>

역시 기하하적으로 살펴보자. 좌변은 $(t, f(t))$를 지나는 직선이고 우변은 함숫값이다. 따라서 위의 theorem이 시사하는 바는 그래프 위의 어느 점에서도 그래프 아래만을 지나는 직선을 항상 찾을 수 있다는 것이다. 만약 $f$가 differentiable 하다면 그 직선은 결국 $f$의 tangent line<sub>접선</sub>인데, 그 어느 tangent line도 convex function 위로 지나갈 수 없음을 의미하기도 한다.


만약 $f$가 두 번 differentiable하다는 사실이 알려져 있고 $\mathcal{C}$에서 그 두 번 미분한 함수가 non-negative라면 $f$가 convex인 것도 알려져 있다.

위의 사실 모두 **Definition 9.1**로 어렵지 않게 증명이 가능하지만, 여기서는 생략한다. 이제 convex 계에서 가장 잘 알려진 inequality 하나를 살펴볼 것이다.

<br>

> **Theorem 9.3 (Jensen's Inequality)** $\mathcal{C}$가 open interval이고 $f$는 그 위에서 정의된 convex function이라고 하자. 또, 이 위에서 정의된 random variable $X$에 대하여(즉, $P(X\in\mathcal{C}) = 1$) $EX\lt\infty$라면,
>
> $$ f(EX) \le E f(X)$$
>
> 가 성립한다. 만약 $f$가 strictly convex하다면 $X$가 a.e. constant가 아닌 이상 위의 inequality는 $x\ne t$에서 equality 없이 strict하게 성립한다.
>
> *Proof*: $t = EX$에 대하여 **Theorem 9.2**에 의해 다음을 만족하는 상수 $c$가 존재한다.
>
> $$ f(EX) + c(x - EX) \le f(x), \quad x\in\mathcal{C}$$ 
>
> 그러므로
>
> $$ f(EX) + c(X - EX) \le f(X), \quad (\text{a.e. } P)$$
>
> 따라서 위에서 양변에 expectation을 씌워주면 원하는 것을 얻을 수 있다.
>
> 만약 $f$가 strictly convex라면 $X\ne t$에 대하여 첫 번째와 두 번째 inequality가 strict하게 성립함을 알 수 있다. Null set을 제외한 모든 점에서 inequality가 strict하므로 expectation을 씌웠을 때도 strict inequality가 성립한다.
>

<br>

Jensen's inequality는 $X$가 더 높은 차원인 random vector에 대해서도 성립한다. 

<br>
<br>

# Rao-Blackwell Theorem

이제 estimator의 이야기로 전한해보자. 이전 post에서 $g(\theta)$의 estimator $\delta(X)$에 대하여 loss function과 risk function을 정의한 바 있다. 또한, 그 어느 data 기반 estimator $\delta(X)$에 대하여, $\delta$와 똑같은 risk를 가지고 sufficient statistic $T$를 기반으로 random number를 곁들인 estimator가 항상 존재한다는 것을 알아보았다(*Sufficiency*편 - **Theorem 6.4** 참조). 우리는 이제 loss가 convex function일 때에는 $\delta$보다 risk가 작고 random number를 쓰지 않는 $T$를 기반으로 하는 estimator가 있음을 보일 것이다.

<br>

> **Theorem 9.4 (Rao-Blackwell)** 주어진 family $\mathcal{P} = \\{P_{\theta}: \theta\in\Omega\\}$에 대하여 $T$가 sufficient statistic이라고 하자. $\delta$가 $g(\theta)$의 estimator라고 했을 때 $\eta(T):=E[\delta(X)\vert T]$를 정의한다($\eta$는 $\theta$와 관련이 없기에 expectation을 계산할 때 $\theta$를 특정지어 줄 필요는 없다). 만약 $R(\theta, \delta)\lt\infty$이고 $L(\theta, \cdot)$이 convex할 때,
>
> $$ R(\theta, \eta) \le R(\theta, \delta), \quad \theta\in\Omega$$
>
> 나아가, $L(\theta, \cdot)$가 strict convex하다면, $\delta(X) = \eta(T)$ (a.e. $P_{\theta}$)가 아닌 이상 위의 inequality도 strict하게 성립한다.
>
> *Proof* : Conditional random variable 형태의 Jensen's inequality를 적용하면 된다. $f(\cdot) = L(\theta, \cdot)$으로 두면
>
> $$ L(\theta, \eta(T)) = L(\theta, E[\delta(X)\vert T]) \le E_{\theta}[L(\theta, \delta(X))\vert T]$$
>
> 양쪽에 expectation을 씌워주면 smoothing과 risk function 정의에 의해 $R(\theta, \eta)\le R(\theta, \delta)$임이 증명된다. 만약 $L$이 strict convex라면,  $\delta(X) = \eta(T)$가 아닌 곳에서는 위의 inequality가 strict하게 성립하므로 결론 부분에서도 $\delta(X) = \eta(T)$ (a.e. $P_{\theta}$)이 아닌 곳에서라면 risk function 사이의 inequality 또한 strict하다.
>

<br>

이 theorem이 궁극적으로 보여주는 것은 만약 convex loss에서(이를 테면 squared loss) estimator의 performance를 오로지 risk function의 크기로만 따진다고 했을 땐 estimator가 굳이 data $X$의 함수일 것까지는 없다는 점이다. Estimator를 구성할 때 sufficient statistic $T$의 함수로만 구성한다면 너무나도 좋은 performance를 가지는 estimator룰 얻을 수 있다. 그리고, 한편으로는 $T$를 함수로 하는 그 어느 randomized estimator는 non-randomized estimator보다 performance가 떨어지는 것을 보여주기도 한다. 

이를 조금 더 확장하여 어느 randomized estimator는 non-randomized estimator보다 performance가 좋지 않음을 증명할 수 있다. Randomized estimator는 data $X$와 data는 전혀 무관한(independent한) random number $U$로 구성한다고 했을 때 만약 $X$와 $U$ 두 개 모두 data로 취급한다면, $\delta(X, U)$는 $\eta(X) = E_{\theta}[\delta(X, U)\vert X]$보다 performance가 좋지 않을 것이다. 그러므로 오히려 random 요소를 제거한 $X$ 자체로만 만든 non-randomized estimator가 더 좋다.

이 모든 논의는 다시 한 번 언급하지만 loss가 convex일 때만 성립한다. 또, estimator의 performance를 risk를 가지고 판단하는 것은 매우 중요하지만 오직 risk 하나만을 두고 performance를 판단하진 않는다(이전에 살펴보았듯, risk function의 대소비교가 parameter의 값에 따라 달라지기도 하고 risk가 estimator의 performance의 모든 것을 설명해주진 않는다). 이에 대해서는 앞으로 끊임없이 의논할 것이다.

<br>
<br>
<br>

---
다음 편에서는 estimator 중에서도 그 expectation이 estimate하고자 하는 parameter $g(\theta)$와 정확히 일치하는 unbiased estimator에 대해서 살펴볼 것이다. Unbiased estimator도 하나로만 특정지어지진 않는데, 그 중에서 가장 좋은 performance를 가지는 estimator도 소개한다.
