---
title: Medians as an Estimator(중앙값의 추정량으로써의 역할)
date: 2024-05-09 19:23:00 +09:00
categories: [Statistics, Mathematical Statistics]
math: true
pin: true
tags:
  [
    probability,
    mathematical statistics,
    median,
    percentile,
    estimator,
    large sample theory,
    asymptotic relative efficiency,
    ARE
  ]
---

지난 글에서는 likelihood에 입각하여 설정한 MLE에 대해서 간단히 다루었다. 그럴듯하게 정의된 MLE에 이어서, 이번에는 median<sub>중앙값</sub>이 estimator로 활용되었을 때 역시 성능 좋은 estimator로 활용될 수 있을지 정량적으로 알아보고자 한다. CLT 등에서 보면 우리는 늘 sample mean에 상당한 관심을 가지고 효율성과 limiting distribution 등 많은 것을 분석하였다. 우리는 data의 outlier<sub>이상치</sub>에 취약하다는 mean이 가지는 단점을 median이 보완할 수 있다는 점을 뻔히 알면서도 median을 estimator로써 활용할 생각을 지금까지 미처 하지 못했던 것이다. 이번 글에서는 그러므로 median을 estimator로 활용할 때 가져오는 장점 등을 파악할 것이다. 그리고 생각보다 median은 mean 못지 않게 실제로 상당히 많이 활용되는데, Bayesain inference에서는 심지어 이 둘의 성능 차이가 거의 없다고 판단하고 median이 더 계산이 편리하다는 이유로 posterior median<sub>사후 중앙값</sub> 등을 estimator로 mean보다도 더 많이 활용한다. 즉, median도 결코 무시해서는 안 될 estimator라는 것이다.

<br>
<br>

# Medians as an Estimator

-1Median을 order statistic을 활용하여 수학적으로 표현해보자. Random sample $X\_1, X\_2, \cdots, X\_n$에 대하여 median $\tilde{X}$는

$$ \tilde{X} = \begin{cases}
X_{(m)}, & n = 2m-1; \\
\frac{1}{2}(X_{(m)}+X_{(m+1)}), & n = 2m.
\end{cases}
$$

로 나타낼 수 있다. 이제 random sample $X\_1, X\_2, \cdots$가 i.i.d.이고 동일한 CDF $F$를 가지고 있다고 하자. 그리고 $\tilde{X}\_n$을 첫 $n$ 개의 sample의 median이라고 두자. 적당한 regularity condition 아래에서 $F$는 unique한 median $\theta$, 즉 $F(\theta) = 1/2$인 $\theta$가 unique하게 존재한다고 하고, 그 $\theta$에 대하여 $F'(\theta)$도 존재한다고 하자. 우리는 이제 다음을 근사하고자 할 것이다.

$$ P(\sqrt{n}(\tilde{X}_n - \theta)\le a) = P(\tilde{X}_n\le \theta +a/\sqrt{n})$$

다음을 정의하자.

$$ S_n = \#\{i\le n: X_i\le \theta+a/\sqrt{n}\}$$

그러면 우리는 다음 두 event가 동일함을 알 수 있다.

$$ \tilde{X}_n\le \theta +a/\sqrt{n} \Longleftrightarrow S_n\ge m$$

또한, 각 $i$에 대하여 $X\_i \le \theta+a/\sqrt{n}$이 성립하면 성공했다는 관점에서 보면, 우리는 

$$S_n\sim B(n, F(\theta+a/\sqrt{n}))$$

임을 알 수 있다. 이제 binomial distribution에 대한 normal distribution 근사를 다음을 근거로 하여 시도할 것이다. 이는 CLT로부터 바로 유도된다.

$$
\begin{align*}
\sqrt{n}\left(\frac{Y_n}{n} - p\right) &= \frac{Y_n - np}{\sqrt{n}}\leadsto N(0, p(1-p)) \\
P\left(\frac{Y_n - np}{\sqrt{n}}\gt y\right) &= 1 - P\left(\frac{Y_n - np}{\sqrt{n}}\le y\right) \\
&\to 1 - \Phi\left(\frac{y}{\sqrt{p(1-p)}}\right) = \Phi\left(\frac{-y}{\sqrt{p(1-p)}}\right), \quad n\to\infty
\end{align*}
$$

이를 활용하면

$$
\begin{align*}
P(\sqrt{n}(\tilde{X}_n - \theta)\le a) & = P(S_n \gt m - 1) \\
&= P\left(\frac{S_n - nF(\theta+a/\sqrt{n})}{\sqrt{n}}\gt\frac{m-1-nF(\theta+a/\sqrt{n})}{\sqrt{n}}\right) \\
&= \Phi\left(\frac{[nF(\theta+a/\sqrt{n})-m+1]/\sqrt{n}}{\sqrt{F(\theta+a/\sqrt{n})(1 - F(\theta+a/\sqrt{n}))}}\right) + o(1) 
\end{align*}
$$

이때 $o(1)$은 $n\to\infty$임에 따라 0으로 converge하는 모든 sequence를 통들어 일컫는 표기방식이다. 이에 대해선 곧 따라오는 글에서 다루게 될 것이다. 무튼, $F$가 $\theta$에서 continuous하기 떄문에

$$\sqrt{F(\theta+a/\sqrt{n})(1 - F(\theta+a/\sqrt{n}))} \to 1/2, \quad n\to\infty $$

이다. 또 $F$는 $\theta$에서 differentiable하기 때문에

$$
\begin{align*}
\frac{nF(\theta+a/\sqrt{n})-m+1}{\sqrt{n}} &= a\frac{F(\theta+a/\sqrt{n}) - F(\theta)}{\sqrt{n}} + \frac{nF(\theta)-m+1}{\sqrt{n}} \\
&= a\frac{F(\theta+a/\sqrt{n}) - F(\theta)}{a/\sqrt{n}} +\frac{1}{2\sqrt{n}} \\
&\to a F'(\theta), \quad n\to\infty 
\end{align*}
$$

이다. 따라서 우리는

$$P(\sqrt{n}(\tilde{X}_n - \theta)\le a)\to \Phi(2aF'(\theta))$$

를 얻고, 이를 다시 표현하면

$$\sqrt{n}(\tilde{X}_n - \theta)\leadsto N\left(0, \frac{1}{4[F'(\theta)]^2}\right)$$

가 된다. 우리는 따라서 median의 limiting distribution을 유도하였다.

비슷한 과정으로 우리는 median뿐만 아니라 다른 quantile<sub>분위수</sub>에 대하여도 얼마든지 limiting distribution을 비슷한 방식으로 유도할 수 있다.

<br>

> **Theorem 21.1** $X\_1, X\_2, \cdots$가 i.i.d. random sample이고 동일한 CDF $F$로부터 추출되었다고 하자. $\gamma\in(0, 1)$에 대하여 $\tilde{\theta}\_n$을 첫 $n$개의 sample $X\_1, \cdots, X\_n$에 대하여 $\lceil\gamma n\rceil$th order statistic이라고 두자(혹은 $\lceil\gamma n\rceil$과 $\lfloor\gamma n\rfloor$th order statistic의 weighted sum<sub>가중합</sub>). $F(\theta) = \gamma$인 $\theta$에 대하여 $F'(\theta)$가 존재하고 finite하며 positive할 때, $n\to\infty$임에 따라
>
> $$\sqrt{n}(\tilde{\theta}_n - \theta) \leadsto N\left(0, \frac{\gamma(1-\gamma)}{[F'(\theta)]^2}\right)$$
>
> 가 성립한다.

<br>

위 theorem은 $\gamma$% 정도의 percentile을 가지는 order statistic에 대해서는 위에서 우리가 거쳤던 과정과 비슷하게 하여 limiting distribution을 충분히 구할 수 있음을 보여준다. Convergence는 $\sqrt{n}$ 정도가 곱해졌을 때 normal distribution으로 이루어짐을 알 수 있다. 하지만 이렇게 비교적 중앙에 위치하는 order statistic의 행동 양상과는 다르게, 항상 꼬리 쪽에 있는 order statistics는 convergence 속도도 $1/\sqrt{n}$이 아닐 수 있는 등의 다른 점이 있다. 이를 다음 예를 통해 구체적으로 살펴보자.

<br>

> **Example 21.2** $X\_1, \cdots, X\_n$이 exponential distribution $\text{Exp}(1)$로부터 얻은 i.i.d. sample이라고 하자. 우리는 두 번째로 작은 order statistics $X\_{(2)}$의 limiting distribution이 궁금하다. 당연하겠지만 $n\to\infty$임에 따라 $X\_{(2)}\overset{p}{\to}$이기 때문에 우리는 적절한 $n$의 power $p$를 곱해주어 $n^p X\_{(2)}$가 0이 아닌 적당한 distribution에 converge하도록 만들고 싶다. 하지만 **Theorme 21.1**에서 관찰한 quantile들의 limiting distribution과는 다르게 $p = 1/2$가 적절한 power는 아니다. 적절한 $p$를 찾기 위해 우리는 $P(n^p X\_{(2)}\le x)$의 converge하는 behaviour를 살펴볼 필요가 있다. 먼저 $X\_{(2)}$의 CDF를 구해보자. PDF는 다음과 같이 주어진다.
>
> $$ f_{X_{(2)}}(x) = \frac{n!}{1!(n-2)!}(1 - e^{-x})(e^{-x})(1 - (1-e^{-x}))^{n-2} = n(n-1)(1-e^{-x})e^{-(n-1)x}, \quad x\ge 0 $$
>
> 그러므로 CDF는 다음과 같이 구할 수 있다.
>
> $$ F_{X_{(2)}}(x) = 1 - ne^{-(n-1)x} + (n-1)e^{-nx} , \quad x\ge 0$$
>
> 이를 활용하면
>
> $$
> \begin{align*}
> P(n^p X_{(2)}\le x) &= F_{X_{(2)}}\left(\frac{x}{n^p})  \\
> &= 1 + (n-1) e^{-\frac{1}{n^{p-1}x}} - ne^{-\frac{n-1}{n^p}x} \\
> &= 1 + ne^{-\frac{1}{n^{p-1}}x}\left(1 - e^{\frac{1}{n^p}x}\right) - e^{-\frac{1}{n^{p-1}}x}\\
> &= 1 - \frac{e^{\frac{x}{n^p}}}{\frac{x}{n^p}}\cdot e^{-\frac{1}{n^{p-1}}x}n^{1-p}x - e^{-\frac{1}{n^{p-1}}x}, \quad x\ge0 \\
> \end{align*}
> $$
>
> 이므로 $p = 1$이 되면 $n\to\infty$임에 따라 위의 식이 $1 - (x+1)e^{-x}$로 수렴하는 것을 알 수 있다. 이렇듯 median하고 전혀 다른 양상을 띔을 알 수 있다.
>
> Order statistics 중 가장 큰 값을 나타내는 $X\_{(n)}$ 또한 사뭇 다른 패턴의 limiting distribution을 가지고 있다. 놀랍게도 우리는 $X\_{(n)} - \log{n}$의 limiting distribution이 다음 이유에서 존재함을 알 수 있다.
>
> $$
> \begin{align*}
> P(X_{(n)} -\log{n} \le x) &= P(X_{(n)} \le x + \log{n}) \\
> &= (1 - e^{-x - \log{n}})^n \\
> &= \left[\left(1 - \frac{e^{-x}}{n}\right)^{-\frac{n}{e^{-x}}}\right]^{-e^{-x}} \\
> &\to \exp(-e^{-x}), \quad x\ge 0, \quad \text{as} \quad n\to\infty
> \end{align*}
> $$
>
> 이 distribution은 log-Weibull이라고 알려진 distribution이다(Weibull distribution을 따르는 random variable에 logarithm을 씌운 것의 distribution이라고 보면 된다).


<br>
<br>


# Asymptotic Relative Efficiency(ARE)<sub>점근상대효율성</sub>

Estimator들의 efficiency를 그들의 limiting distribution을 활용하여 비교하는 방법이 있다. 바로 limiting distribution의 variance를 비교하는 방벙이다. Sample size을 아무리 키워도 점근적으로 보았을 때 이 estimator가 가지는 어느 정도의 variance가 있을 것이다. 이 variance가 sample size아 무한히 커져도 아직 상당히 큰 값을 유지하는 상태라면 그 estimator는 그닥 성능이 좋지 않음을 직관적으로 알 수 있다.

이 개념을 바로 살펴보기 전에 먼저 sample mean과 median을 비교해보고자 한다. 이 둘을 estimator로써 비교하기 위해서는 먼저는 mean과 median이 같은 parameter를 대상으로 하는 estimator여야 한다는 가정이 필요하다. 이러한 환경을 가정하기 위해서는 data $X\_1, \cdots, X\_n$이 location family부터 i.i.d.하게 추출되었다고 하는 것이 가장 간편하다. 이때 location family라는 것은 parameter가 distribution을 평행이동시키는 역할을 하는 family를 일컫는다. 예컨대 uniform distribution 중에서도 $\mathcal{U}(\theta, \theta+1)$같은 것들은 parameter $\theta$가 distribution을 평행 이동시키는 역할을 하는 것을 알 수 있으며 모양 등을 따로 변화시키진 않는다. 또 다른 예로는 $N(\mu, 1)$ 등이 있겠다. 그리고 location family에서의 parameter를 *location parameter*<sub>위치모수</sub>라고 부른다.

무튼 여기서는 data가 location family를 따르고 density $f(x - \theta)$를 따른다고 가정할 것이다. 또 $f$는 0에서 symmetric하다고 가정할 것이다. 그러므로 모든 density는 parameter $\theta$에 대하여 symmetric한 것이므로 $P\_{\theta}(X\_i\lt\theta) = P\_{\theta}(X\_i \gt\theta) = 1/2$이고 $E\_{\theta}X\_i = \theta$임을(만약 존재한다면 말이다) 알 수 있다. 그럼 여기서 sample mean과 sample median 모두 $\theta$에 대한 estimator로 고려할 수 있다.

Sample mean의 limiting distribution의 경우, CLT에 의하여

$$\sqrt{n}(\overline{X}_n - \theta)\leadsto N(0, \sigma^2)$$

이다. 이때 

$$\sigma^2 = \int x^2 f(x)\, dx$$

이다. 그리고 sample median의 limiting distribution는 **Theorem 21.1**에 의하여

$$\sqrt{n}(\tilde{X}_n - \theta)\leadsto N\left(0, \frac{1}{4f^2(0)}\right)$$

가 성립한다. 위에서 우리는 자연스럽게 $F'(0) = f(0)$임을 가정했다. 

조금 더 명확한 비교를 위하여 $f$가 standard normal distribution의 density라고 가정하자. 즉, $f(x-\theta)$는 $N(\theta, 1)$의 density일뿐이다. 그러면 $\sigma^2 = 1$이고 $1/(4f^2(0)) = \pi/2$이다. 여기서 median이 mean보다 limiting distribution의 variance가 약간 더 큰 것을 알 수 있다. 따라서 normal location family에 대해서는 sample mean이 조금 더 좋은 performance를 가지고 있다고 봐야겠다. 이를 다르게 표현하면 sample median이 sample mean보다 덜 efficient하다는 이야기이다. '덜 efficient'하다는 것이 무엇을 의미하는지 조금 더 살펴보도록 하자.

$m = m\_n = \lfloor \pi n/2\rfloor$이라고 두면 우리는 $n\to\infty$임에 따라 $\sqrt{n/m}\overset{p}{\to} \sqrt{2/\pi}$임을 알 수 있다. Convergence in proability와 convergence in distribution을 곱하여도 convergence in distribution이 유지되므로(Mathematical Statistics - Several Types of Convergence, **Theorem 19.15**참조),

$$\sqrt{n}(\tilde{X}_m - \theta) = \sqrt{\frac{n}{m}}\sqrt{m}(\tilde{X}_m - \theta)\leadsto N(0, 1)$$

을 만족한다. $N(0, 1)$은 sample mean의 limiting distribution이었음을 상기하라. 이것이 결국 보여주는 것은 $m$개의 관측치로 구성한 sample median이 가지는 error는 $n$개의 관측치로 구성한 sample mean이 가지는 error과 비슷하다는 것이다. 그리고 $n\to\infty$임에 따라 $m/n\to \pi/2$이므로, 결국 sample median이 sample mean이 가지는 error 수준까지 다가가기 위해서는 더 많은 관측치의 개수를 필요로 하다는 것이다. 그리고 더 필요한 관측치는 sample mean에 비하여 $\pi/2$배 정도임을 명시한다. 그러므로 $m/n$의 limit $\pi/2$를 $\tilde{X}\_n$에 대한 $\overline{X}\_n$의 *asymptotic relative efficiency*, 줄여서 *ARE*라고 부른다. 

일반적으로 $n$개의 sample로 계산되는 공통된 parameter $\theta$에 대한 두 estimator $\hat{\theta}_n$과 $\tilde{\theta}_n$에 대하여 다음과 같이 limiting distribution을 가진다고 할 때,

$$
\begin{align*}
\sqrt{n}(\hat{\theta}_n - \theta)&\leadsto N(0, \sigma_{\hat{\theta}}^2) \\
\sqrt{n}(\tilde{\theta}_n - \theta) &\leadsto N(0, \sigma_\{\tilde{\theta}}^2)
\end{align*}
$$

우리는 $\tilde{\theta}\_n$에 대한 $\hat{\theta}\_n$의 ARE룰 $\sigma\_{\hat{\theta}}^{-1}/\sigma\_{\tilde{\theta}}^{-1} = \sigma\_{\tilde{\theta}}^2/\sigma\_{\hat{\theta}}^2$라고 말할 수 있다. 이 ARE는 앞에서도 살펴보았지만 서로 비슷한 수준의 error를 갖기 위해 각각이 필요한 sample의 개수에 대한 비율이라고도 볼 수 있다.

앞에서 사실 $N(\theta, 1)$에 대하여 sample mean과 median을 비교했을 때, sample mean이 UMVUE였기 때문에 사실상 sample mean이 더 efficient할 것이라는 점은 기정사실화되어 있었다. 대신에 만약

$$ f = \frac{1}{2}e^{-\vert x\vert}$$

를 고려하면, 먼저

$$\sigma^2 = \int x^2 \cdot \frac{1}{2}e^{-\vert x\vert}\, dx = \int_0^\infty x^2 e^{-x}\, dx = \Gamma(3) = 2$$

이므로 각각의 limiting distribution은 다음과 같이 얻어진다.

$$
\begin{align*}
\sqrt{n}(\overline{X}_n - \theta)&\leadsto N(0, 2) \\
\sqrt{n}(\tilde{X}_n - \theta)&\leadsto N(0, 1) \\
\end{align*}
$$

여기서는 놀랍게도 sample median이 sample mean보다 두 배나 efficient, 즉 mean에 대한 median의 ARE가 2나 됨을 알 수 있다. 다시 말해, sample mean이 sample median 정도의 efficiency를 가지려면 median보다 두 배나 많은 sample을 필요로 함을 알 수 있다. 이 경우는 사실 sample median이 MLE인 경우(쉽게 구할 수 있다)로, 나중에 MLE는 가장 efficient한 estimator로 뛰어난 performance를 자랑하고 있음을 다룰 것이다.

<br>

> **Example 21.3** Data $X\_1, \cdots, X\_n$이 N(\theta, 1)$로부터 i.i.d하게 추출된 random sample이라고 하자. 우리는 다음을 estimate하고 싶다.
>
> $$ p:= P_{\theta}(X_i \le a) = \Phi(a - \theta)$$
>
> 이때 $\Phi$는 normal distribution의 CDF이다. 이를 위한 가장 명확한 estimator 중 하나로 MLE를 생각할 수 있다. $\theta$에 대한 MLE는 $\overline{X}\_n$이므로 $p$에 대한 MLE는
>
> $$\hat{p} = \Phi(a - \overline{X}_n)$$
>
> 이다. 또다른 관점으로 자연스럽게 생각할 수 있는 estimator는 실제로 $a$보다 작거나 같은 data의 개수를 count하는 것이다. 즉,
>
> $$ \tilde{p} =\frac{1}{n}\#\{i\le n: X_i\le a\} = \frac{1}{n}\sum_{i=1}^n I\{X_i \le a\}$$
>
> $\tilde{p}$에 대한 limiting distribution은 CLT로 금방 구할 수 있다. 즉, 
>
> $$\sqrt{n}(\tilde{p} - p)\leadsto N(0, \tilde{\sigma}^2) $$
>
> 이때 
>
> $$\tilde{\sigma}^2 = Var_{\theta}(I\{X_i \le a\}) = \Phi(a - \theta)(1 - \Phi(a - \theta)) $$
>
> 첫 번째 estimator $\hat{p}$에 대한 limiting distribution은, CLT로 $\overline{X}$의 limiting distribuiton을 알 수 있으므로 거기에 delta method을 적용하면 된다. 즉,
>
> $$ \sqrt{n}(\hat{p} - p)\leadsto N(0, \hat{\sigma}^2) $$
>
> 이때
>
> $$\hat{\sigma}^2 = \left[\frac{d}{dx}\Phi(a - x)\bigg\vert_{x = \theta}\right]^2 = \phi^2(a - \theta)$$
>
> 그러므로 $\tilde{p}$에 대한 $\hat{p}$의 ARE는
>
> $$ \frac{\Phi(a - \theta)(1 - \Phi(a-\theta))}{\phi^2(a-\theta )}$$
>
> 임을 알 수 있다. 여기서의 ARE는 운이 나쁘게도 unknown parameter $\theta$에 따라 달라진다는 단점이 있다. $\theta= a$인 경우, $\text{ARE} = \pi/2$인데, 이를 기점으로 $\vert \theta - a\vert$가 증가하면 ARE도 덩달아 증가한다는 성질이 있다. 그러므로 대체로 $\hat{p}$가 더 좋은 perfomance를 지님을 알 수 있다. 하지만, 유의할 점이 한 가지 있는 것은 $\tilde{p}$는 우리가 활용한 model인 normal distribution이 틀려도 어차피 $X\_i\le a$를 만족하는 data point의 개수 정도만을 새는 estimator로써 영향이 별로 없다. 하지만 $\hat{p}$의 경우 model이 틀려버리면 estimator로써의 정당성을 잃어버린다. 이렇게 $\hat{p}$처럼 어떤 model를 활용하냐에 따라 결과가 심하게 달라지는 경우를 *robustness가 떨어진다*고 표현한다. 우리는 단순히 efficiency를 최소화하는데 집중할 것이 아니고 model의 선택 자체도 우리의 재량이라는 것을 염두에 두어 estimator의 model에 따른 robustness와 잘 균형을 맞추어 선택해야 할 것이다.
>
>

<br>
<br>

---

다음 편에서는 MLE에 대한 본격적인 분석을 할 것이다. 먼저는 MLE 분석을 위한 수학적인 내용을 정립 후에 convergence, limiting distribution 등을 살펴볼 것이다. 이로써 MLE가 얼마나 좋은 efficiency를 가지고 있는지도 함께 살펴보도록 한다.