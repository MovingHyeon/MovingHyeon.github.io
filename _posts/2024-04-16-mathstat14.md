---
title: Conditional Distributions
date: 2024-04-16 00:05:00 +09:00
categories: [Statistics, Mathematical Statistics]
math: true
pin: true
tags:
  [
    probability,
    mathematical statistics,
    conditional distributuion,
    product measure,
    product algebra
  ]
---


이전 편에서는 conditional distribution 정의를 수학적으로 정립하기 위해 기본적인 도구들을 다루었다. 그중 대표적으로 다루었던 product measure는 joint distribution으로부터 joint density 등을 잘 정의할 수 있도록 도와주며, Fubini's theorem은 joint distribution의 probability 등 활용도가 굉장히 높다. 이번 편에서는 저번 편에서 소개한 수학적 도구들로 conditional distribution을 정의해본다.

기호를 조금 단순화하여, 지난 글에서는 product measure를 $\mu\otimes\nu$라고 표기했었는데, 여기서는 $\mu\times\nu$로 표현하고자 한다.

<br>
<br>

# Joint and Marginal Densities

$X$와 $Y$를 각각 $\mathbb{R}^m$과 $\mathbb{R}^n$에서의 random vector이라고 하고, joint random vector $Z = (X\ Y)^T \in\mathbb{R}^{n+m}$을 정의하자. Distribution $P_Z$가 $\mu\times\nu$에 대하여 density $p_Z$를 가진다고 하자. 이때 $\mu$와 $\nu$는 각각 $\mathbb{R}^m$과 $\mathbb{R}^n$의 measure이다. 이러한 $p_Z$를 $X$와 $Y$의 *joint density*라고 부른다. $Z$에 관한 probability는 다음과 같이 계산된다.

$$P(Z\in B) = \iint 1_B(x, y)p_Z(x, y)\,d(\mu\times\nu)(x, y)$$

위의 적분을 계산할 때는 Fubini's theorem을 활용하면 편리하며, 어느 변수의 적분을 먼저 하든 상관없다. 

만약 joint distribution으로부터 $X$ 하나만의 probability를 계산하고 싶으면, $X$에 대해서는 원하는 event 내에서만 적분해주고 $Y$는 전체 영역 위에서 적분해주면 된다. 즉, $X\in A$에서의 probability는 $Z\in A\times\mathbb{R}^n$에서의 probability와 동일하다. $1\_{A\times\mathbb{R}^n}(x, y) = 1\_A(x)$이라는 점을 이용하면,

$$
\begin{align*}
P(X\in A) = P(Z\in A\times\mathbb{R}^n) &= \iint 1_A(x)p_Z(x, y)\,d\nu(y)\,d\mu(x) \\
&= \int_A \left\{\int p_Z(x, y)\,d\nu(y)\right\}\,d\mu(x)
\end{align*}
$$

위에서 마지막 식에서 중괄호로 둘러싸인 항은 $X$의 $\mu$에 대한 density로 볼 수 있다. 따라서 $X$의 density는 $\mu$에 대하여 다음과 같이 구할 수 있다.

$$p_X(x) = \int p_Z(x, y)\,d\nu(y)$$

이를 $X$의 *marginal density*라고 한다. 비슷하게 $Y$의 marginal density $p_Y$를 구할 수 있으며, 이는 $\nu$에 대한 density일 것이다.

$$p_Y(y) = \int p_Z(x, y)\,d\mu(x) $$

<br>
<br>

# Conditional Distributions

Conditional distribution을 직접적으로 정의하기 전, 먼저 그 갈피를 잡아보자. $X$와 $Y$가 random vector라고 하자. $X = x$가 주어졌을 때의 $Y$의 conditional distribution $Q_x$는 우리가 익히 사용하던 smoothing을 통해 직관적으로 살펴볼 수 있다. $E\vert f(X, Y)\vert\lt\infty$라면, 다음이 smoothing에 의해 성립한다.

$$ Ef(X, Y) = EE[f(X, Y) \vert X] $$

그러므로 $H(X) = E[f(X, Y)\vert X]$으로 $X$만의 함수로 표현이 되고

$$ H(x) = E[f(X, Y)\vert X = x] = \int f(x, y)\,dQ_x(y) $$

그러면 우리는 $Ef(X, Y)$를 다음과 같이 다시 표현할 수 있다.

$$ Ef(X, Y) = \int H(x)\,dP_X(x) = \iint f(x, y)\,dQ_x(y)\,dP_X(x)$$

그러므로 모든 Borel rectangle $A\times B$에 대하여 $f = 1\_{A\times B}$로 두면 우리는 $P(Z\in A\times B)$에 대해 충분히 잘 정의할 수 있다. 이를 바탕으로 conditional distribution을 정의해보자.

<br>

> **Definition 14.1** $X$가 주어졌을 때 $Y$의 *conditional distribution* $Q$는
>
> $$ Y\vert X = x \sim Q_x$$
>
> 로 표현되며, 다음을 만족하는 함수이다.
>
> 1. 모든 $x$에 대하여 각각의 $Q_x(\cdot)$은 probability measure이다.
> 2. 모든 Borel set $B$에 대하여 $Q_x(B)$는 $x$에 대한 measurable function이다.
> 3. 모든 Borel rectangle $A\times B$에 대하여
>
> $$P(X\in A, Y\in B) = \int_A Q_x(B)\,dP_X(x) $$
>
> 를 만족한다.


<br>

만약 $X$와 $Y$가 모두 random variable이라면, 위를 만족하는 conditional distribution이 항상 존재함이 알려져 있다. 하지만 위처럼 conditional distribution을 정의하면 그 의미는 파악이 가능해도, condiitonal distribution을 구하는 지표에 대해서는 알려주지 않는다. 즉, 위는 conditional distribution이 되기 위한 아주 기초적인 조건을 나열했을 뿐, 구체적인 형태까지 알려주진 못한다. 다음 정리는 어떻게 정의하면 $\mu\times\nu$에 대한 conditional density가 될 수 ㅣㅇ;ㅆㄴ잡는다.

<br>

> **Theorem 14.2** 만약 $X, Y$가 random vector이고, $\mu\times\nu$에 대하여 density $p_Z$를 가진다고 하자. 그리고 $p_X$가 $X$의 $\mu$에 대한 marginal density라고 하고, $E:=\\{x: p_X(x)>0\\}$라고 정의한다.
> 
> 각각의 $x\in E$에 대하여
>
> $$p_{Y\vert X}(y\vert x) := \frac{p_Z (x, y)}{p_X(x)}$$
> 
> 를 정의하고 $Q_x$를 $\nu$에 대한 density가 $p_{Y\vert X}(\cdot\vert x)$가 되도록 잡는다. 
> 
> 만약 $x\not\in E$인 경우에는 $p_{Y\vert X}(y\vert x) = p_0(y)$으로 정의하는데, 이때 $p_0$는 임의의 고정된 probability distribution $P_0$의 density이다. 이때는 $Q_x = P_0$로 잡는다. 
> 
> 그러면 $Q$는 $X$가 주어졌을 때의 $Y$의 conditional distribution이다.
>
> *Proof:* Conditional distribution의 조건 **Definition 14.1**에 입각혀여 살펴보면, 첫 번째 조건과 두 번째 조건은 **Theorme 14.2**에서 정의한 것으로부터 당연하게 비롯된다. 세 번째 조건을 살펴보자. $P(X\in E) = 1$이므로, 일반성을 잃지 않고 $x\not\in E$에 대해서는 그냥 아무거나 $p_Z(x, y) =0$정도로 두자. 그러면
>
> $$ p_{Y\vert X}(\cdot \vert x) = \frac{dQ_x}{d\nu} $$
> 
> 
> 가 성립한다. 그리고 단지 $E$ 위에서만이 아니라 모든 $x$ 대하여 $p_Z(x, y) = p_X(x)p_{Y\vert X}(y\vert x)$ 가 성립한다(물론 모든 $y$에 대해서도 말이다). 그러면 세 번째 조건에서의 우변은
>
> $$
> \begin{align*}
> \int_A Q_x(B)\,dP_X(x) &= \int_A \int_B \,dQ_x\,dP_X \\
> &= \int_A\int_B p_{Y\vert X}(y\vert x)\,d\nu(y)\, p_X(x)\,d\mu(x) \\
> &= \iint_{A\times B} p_Z(x, Y)\,d\nu(y)\,d\mu(x) \\
> &= P(X\in A, Y\in B)
> \end{align*}
> $$
>
> 으로 좌변과 동일함을 알 수 있다. 따라서 위에서 정의한 $p_{Y\vert X}$는 conditional distribution이라고 할 수 있다.

<br>

만약 $X$와 $Y$가 independent하다면 $p_Z(x, y) = p_X(x)p_Y(y)$가 성립함이 알려져 있으므로 

$$p_{Y\vert X}(y\vert x) = \frac{p_X(x)p_Y(y)}{p_X(x)} = p_Y(y),\quad (\text{a.e.} x)$$

가 만족된다. 따라서 $Y$의 $X$에 대한 conditional distribution과 marginal distribution이 동일함을 알 수 있다.

<br>

> **Example 14.3** $\mu$가 $\\{0, 1, \cdots, k\\}$에서의 counting measure이고 $\nu$가 $\mathbb{R}$ 위에서의 Lebesgue measure라고 하자. 다음의 joint density를 정의하자.
>
> $$p_Z(x, y) = \begin{cases} 
> \displaystyle \binom{k}{x} y^x (1-y)^{k-x}, & x = 0, 1, \cdots, k, y\in(0, 1) \\
> 0, & \text{otherwise}
> \end{cases}
> $$
>
> 그러면 $X$와 $Y$ 각각의 marginal density는 다음과 같이 구할 수 있다.
>
> $$
> \begin{align*}
> p_X(x) &= \int_0^1 \binom{k}{x} y^x (1-y)^{l-x}\,dy = \binom{k}{x}\frac{x!(k-x)!}{(k+1)!}\frac{1}{k+1}, \quad x = 0, 1, \cdots, k \\
> p_Y(y) &= \begin{cases} \displaystyle\int p_Z(x, y)\,d\mu(x) = \sum_{x=0}^k \binom{k}{x}y^x(1-y)^{k-x} = 1, & y\in (0, 1) \\
> 0, & y\not\in (0, 1) \\
> \end{cases}
> \end{align*}
> $$
>
> 그러므로 $X$는 $\\{0, 1, \cdots, k\\}$ 위에서 uniform, $Y$는 $(0, 1)$ 위에서 uniform하게 distributed되어 있음을 알 수 있다. 이제 이들의 conditonal distribution을 구해보자. 먼저 $Y$가 주어졌을 때 $X$의 conditional distribution은 **Theorem 14.2**에서 정의한 condiitonal distribution에 입각하여 계산하면
>
> $$ p_{X\vert Y}(x\vert y) = \frac{p_Z(x, y)}{p_Y(y)} = \binom{k}{x} y^x (1-y)^{k-x}, \quad x = 0, 1, \cdots, k$$
>
> 이다. 이 함수는 $y$는 고정된 채 $x$에만 관한 함수임을 참고하라. 그러면 위의 mass function은 성공할 probability가 $y$일 때 $k$번 시행하는 binomial distribution이 가지는 counting measure에 대한 density임을 알 수 있다. 그러므로
>
> $$X\vert Y = y \sim B(k, y)$$
>
> 로 표기할 수 있다. 비슷하게 $X$가 주어졌을 때 $Y$의 conditional distribution을 계산하면
>
> $$p_{Y\vert X}(y\vert x) = \frac{p_Z(x, y)}{p_X(x)} = (k+1)\binom{k}{x}y^x(1-y)^{k-x} = \frac{\Gamma(k+2)}{\Gamma(x+1)\Gamma(k-x+1)}y^{x}(1-y)^{k-x},\quad y\in (0, 1)$$
>
> 위의 density는 beta distribution이 가지는 density임을 알 수 있다. 그러므로
>
> $$ Y\vert X = x \sim \text{Beta}(x+1, k-x+1)$$
>
> 로 표현할 수 있다. 이렇게 conditional distribution이 대표적인 distribution을 따를 때 smoothing을 활용하여 각 random variable의 expectation이나 variance를 구할 수 있다. 이를 테면 $X$의 expectation과 variance를 먼저 구해보자. $X\vert Y$는 binomial distribution을 따르므로
>
> $$ E[X\vert Y] = kY$$
>
> 를 만족한다. 따라서 smoothing에 의해
>
> $$EX = EE[X\vert Y] = E[kY] = k\int_0^1 y\,dy = \frac{k}{2}$$
>
> 이는 직접 계산한 경우와 동일한 결과임을 알 수 있다.
>
> $$EX = \sum_{x=0}^k \frac{x}{k+1} = \frac{k}{2}$$
>
> 마찬가지로 binomial distribution에서의 2차 moemnt가 $ky(1-y) + k^2y^2$임을 이용하면
>
> $$EX^2 = EE[X^2\vert Y] = E[kY(1-Y)+k^2Y^2] = \int_0^1 (ky(1-y) + k^2y^2)\,dy = \frac{k(1+2k)}{6} $$
>
> 가 성립한다. 이는 직접 계산한 것과 역시 결과가 일치한다.
>
> $$EX^2 = \sum_{x=0}^k \frac{x^2}{k+1} = \frac{k(1+2k)}{6}$$
>
> 사실 이 예시에서 활용된 density들은 Bayesian analysis에서 binomial model를 구성할 때 많이 활용한다. Bayesian에서는 probability를 어떤 information의 uncertainty<sub>불확실성</sub>으로 받아들이기 때문에, Frequentist가 $\theta$의 true value가 있다고 생각하고 sampling을 여러 번 하면서 그 true value를 기준으로 estimation을 하는 것과는 달리, $\theta$가 가질 수 있는 값의 uncertainty를 probability로 가정해야 하기에 기존에 $\theta$에 관한 알고 있던 information을 바탕으로 우선 $\theta$에 대한 distribution을 가정해야 한다. 그리고 data를 한 번 얻은 후 그 data를 바탕으로 $\theta$에 대한 uncertainty, 즉 distribution을 update하는 방식으로 이루어진다. 하지만 만약 $\theta$에 대한 아무런 information을 알고 있지 않을 때는 모든 가능한 값이 동등한 uncertainty를 가지고 있다고 하고 uniform distribution으로 가정한다. 이 예시에서는 $Y$가 parameter라고 생각하면 편하다. 만약 data $X$의 형태가 binomial model를 적용하는 것이 가장 적합해보이면 binomial distribution을 $X\vert Y$에 적용하면 된다. Data는 항상 어떤 고정된 동일한 parameter $Y=y$에서 얻어지므로 이렇게 conditioning을 해주는 것이다. 이렇게 하여 parameter $Y$에 대한 정보를 update해주어 $Y\vert X$의 distribution을 얻게 되는데, data $X$에 기반하여 $Y$의 distribution을 수정한 것이기 때문에 $X$를 conditioning해준 것이다. 만약 $Y$의 처음 distribution이 uniform으로 가정되었고, binomial modeling을 해준다면 update된 $Y$의 distribution이 Beta distribution이 된다는 것은 Bayesian analaysis에서 굉장히 자주 등장하는 하나의 형태이다. 이와 관련한 자세한 이야기는 Bayesian Inference series 편에 upload하고자 한다.

<br>
<br>

# Building Models

Conditional distribution을 활용한다면 정말 다양한 구조로 modeling할 수 있다. 그중 대표적인 것이 바로 time series data에서의 modeling이다. 다음 예를 살펴보자.

> **Example 14.4** Data가 시간이라는 변수에 따라 변화하는 형태를 가지는 경우가 통계학에서 정말 많이 찾아볼 수 있다. Data가 시간에 따라 측정되는 경우를 특별히 취급하는 이유는 data가 시간의 흐름에 따라 어떤 correlation을 갖는 등 시간과 깊이 연관되기 때문이다. 따라서 시간이라는 변수를 다른 변수와 independent한 별개의 변수로 취급할 수 없으며, 시간에 따른 data의 경향을 잘 살펴보아야 한다. 이러한 data를 time series data<sub>시계열 데이터</sub>라고 부르는데, 대표적인 예로는 stock price<sub>주가</sub> data, 식물/동물/사람 성장 data, 기상 data 등이 있다. 만약 data가 시간 순서에 따라 $X_1, X_2, \cdots, X_n$으로 observe되었다고 하자. 시간에 따라 영향을 받는 data이기 대문에 $X_k$라는 data는 그 이전에 얻어진 data $X_1, \cdots, X_{k-1}$가 어떻게 주어졌냐에 따라 distribution이 달라질 것이다. 이를 테면, $X_1, \cdots, X_n$이 stock price data이면, 사전 stock price $X_1, \cdots, X_{k-1}$가 어떻냐에 따라 $X_k$의 distribution이 결정될 것이므로 다음과 같이 modeling하는 것이 합리적인 것 같다.
>
> $$ X_k\vert X_1 =x_1, X_{k-1} = x_{k-1} \sim N(x_{k-1}+\mu, \sigma^2)$$
>
> 만약 첫째로 수집된 data가 $X\_1 \sim N(x\_0+\mu, \sigma^2)$으로부터 수집되었다면, 전체 data의 joint density는
>
> $$ p_{X_1}(x_1)p(x_1\vert x_1)\cdots p(x_n\vert x_1, \cdots, x_{n-1}) = \prod_{k=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x_k-x_{k-1}-\mu)^2}{2\sigma^2}\right]$$
>
> 이다. 지금까지 우리가 joint density를 계산할 때는 $X_i$ 각각이 independent하다는 조건이 있었기에 각 $X_i$들의 density를 모두 곱했지만, 이제는 $X_i$끼리 independent하지 않기 때문에 그럴 수 없다. 대신에 일반적인 방법인 conditional distribution들끼리의 곱으로 joint를 구할 수 있다.
>
> 가만 살펴보면 위의 density는 $X\_k - X\_{k-1}$가 i.i.d.하게 $N(\mu, \sigma^2)$로부터 추출된 것처럼 보인다. 혹은, *바로 전 단계를 기준*으로 항상 일정한 normal distribution 내애서 random하게 다음 단계를 결정한다. 이러한 형태의 model을 *random walk*라고 한다.
>
> 또 다르게 time series data를 위해 구성할 수 있는 model은 다음과 같다.
>
> $$X_k \vert X_1 = x_1, \cdots, X_{k-1} = x_{k-1} \sim N(\rho x_{k-1}, \sigma^2)$$
>
> 이는 time series model에서 훨씬 많이 쓰이는 가장 기본적인 형태의 model이다. 만약 첫 번째 data가 $X_1\sim N(\rho x_0, \sigma^2)$로 수집되었다면, 이 data $(X_1, \cdots, X_n)$의 joint density는
>
> $$p_{X_1}(x_1)p(x_1\vert x_1)\cdots p(x_n\vert x_1, \cdots, x_{n-1}) = \prod_{k=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x_k-\rho x_{k-1})^2}{2\sigma^2}\right]$$
>
> 가 된다. $\epsilon\overset{i.i.d.}{\sim}N(0, \sigma^2)$의 error term을 도입하면 위 모델은 다음과 같이 다시 쓸 수 있다.
>
> $$X_{i+1} = \rho X_i + \epsilon $$
>
> 이를 하나의 regression이라고 생각하면 $\rho$는 coefficient로 볼 수 있고 $\epsilon$은 error term이 된다. 그러므로 이는 스스로가 시간이 한 unit 더 흐르게 되면 그전 data가 coefficient $\rho$만큼의 비율로 영향을 주게 되는 regression 구조이며, 이를 *autoregression model*<sub>자기회귀모델</sub>이라고 한다. 이 model의 경우는 어떤 상태에서의 data가 바로 그 전 단계의 data에만 영향을 받게끔 modeling되어있으며, 이를 lag<sub>??</sub>가 1인 autoregression model이라고 한다. 만약에 lag가 2면 첫 번째와 두 번째 unit 전 data가 현재의 data에 영향을 준다고 가정한 격이 되며,
>
> $$ X_k = \rho_1 X_{k-1} + \rho_2 X_{k-2} + \epsilon$$
>
> 의 형태가 될 것이다.

<br>

이제 실질적인 분석에서 쓰이는 아주 간단한 model의 예를 살펴볼 것이다. 

<br>

> **Example 14.5** 유행병에 관한 modeling을 할 때에는 시간에 따른 dependence가 아주 큰 역할을 한다. 그러므로 이전 data를 conditioning하여 modeling을 하는 시도는 굉장히 자연스러운 접근이라고 할 수 있다. 여기에서는 이 아이디어를 바탕으로 가장 간단한 형태의 model를 제안하고자 한다. 이 model로부터 가정을 더 완화하거나 더 많은 요소를 고려하면서 model를 발전해나가는 것은 유행병 확진 규모를 예측하는 데 큰 도움이 될 것이다.
>
> $N$을 관심있는 population의 전체 수라고 하고, $X_i$는 $i$ 단계에서 유행병에 확진이 된 사람의 수라고 하자. Model의 simplicity를 위해서, 여기서는 한 번 확진된 사람은 영원히 확진되어 있다고 가정할 것이다. 그리고 각각의 시간 unit마다 확진된 사람이 확진이 아직 안 된 사람에게 전염시킬 probability는 $p = 1-q$라고 가정한다. 그리고 이렇게 전염시키는 과정은 모두 independent하다. 
>
> 위의 가정 하에, $X_k = x_k$가 주어진 상황에서 확진되지 않은 어떤 한 사람이 다음 시간 unit에서도 확진되지 않을 probability는 확진된 사람 $x_k$명으로부터 모두 확진이 되지 말아야 하므로 $q^{x\_k}$라고 할 수 있다. 그러므로 $k$ 단계에서 $(k+1)$단계로 넘어갈 때 새롭게 확진되는 사람의 수 $X_{k+1} - X_k$는 전체 확진되지 않은 사람 $N - x_k$명 중 확진될 probability $1 - q^{x\_k}$를 가지므로 binomial model를 따른다고 두는 것이 가장 합리적이다. 즉,
>
> $$X_{k+1} - X_k \vert X_1 = x_1, \cdots, X_k = x_k \sim B(N-x_k, 1 - q^{x_k})$$
>
> 이다. 즉, 각각의 관측치는 다음의 mass function을 갖는 conditional distribution을 가지고 있다.
>
> $$ p(x_{k+1}\vert x_1, \cdots, x_k) = \binom{N-x_k}{x_{k+1}-x_k}(1-q^{x_k})^{x_{k+1}-x_k}(q^{x_k})^{N-x_{k+1}}$$
>
> 그리고 전체 data의 joint density를 구하고 싶으면 위의 conditional density를 $X_1, \cdots, X_n$에 대해서 모두 곱하면 된다.


<br>
<br>

---
Conditional distribution의 정의를 수학적으로 모두 정립하였다. 이 measure theoretical한 접근으로 conditional distribution을 정의하면 factorization theorem을 증명하는 데 적극적으로 활용할 수 있다. 특히 joint density가 있을지 장담이 되지 않는 상황에서는 이 conditional distribution을 활용하는 것이 매우 좋다. 다음 편에서는 이들을 활용하여 factorization theorem을 measure thoery의 시각에서 증명해보기로 한다. 

