---
title: Unbiased Estimators
date: 2024-04-03 16:59:00 +09:00
categories: [Statistics, Mathematical Statistics]
math: true
pin: true
tags:
  [
    probability,
    mathematical statistics,
    unbiased,
    bias,
    minimum variance,
    minimum variance unbiased estimators,
    UMVUE
  ]
---

<br>
<br>
<br>

<center><div style="width:48%"><i>
I am too familiar with the manner in which actual data are met with the suggestion that other data, if they were collected, might show something else to believe it to have any value as an argument. "Statistics on the table, please," can be my sole reply. <br>

-- Karl Pearson, 1910
</i></div></center>

<br>
<br>

Frequentists는 estimator 중에서도 unbiased estimator에 굉장히 집착한다. Unbiased estimator라고 하는 것은 estimator 중에서도 그 expectation이 예측하는 대상과 정확히 일치하는 것을 말한다. 그래도 estimate를 할 때면 평균적으로 우리가 예측하고자 하는 true value로 나오는 것이 평균적으로 조금 크다거나 평균적으로 조금 작게 biased 되어 estimate되는 것보다 훨씬 좋다고 여겨지는 것이다. 사실 우리가 직관적으로 생각해도, sample 마다 여러 estimated value가 나올 텐데, 이들이 대체로 평균적으로는 true value로 나오는 것이 더 완벽해보이지 않는가? 또, 우리가 지난 post에서 다루었듯, parameter의 true value가 무엇이냐에 따라 risk function의 대소 관계가 달라지므로 어떤 estimator를 사용해야 하는지 명확하지 않았다. Parameter의 true value는 모르는 값이므로 이 값에 따라 estimator를 달리 정하겠다는 것이 불가능하다는 점이었다. 하지만 unbiased estimator는 이 문제점을 해결해준다. 두 unbiased estimator를 비교할 때는 risk function의 대소 관계가 true value에 따라 달라지는 등의 문제가 발생하지 않기 때문이다. Frequentist는 그래서 unbiased estimator에 특히 관심을 보였고, 여러 unbiased estimator 중에서 가장 좋은 것은 무엇일지 연구했다. 

그럴듯하게 보이는 unbiased estimator는 사실 bias와 unbiased 만을 고려했을 때만 좋게 보일 뿐이지, 실상은 뒷따르는 다른 여러 성질과 맞물리는 부분 때문에 unbiased estimator가 무조건 좋은 것은 아닐 때도 있다. 그렇다고 하여 unbiased estimator가 중요하지 않은 것은 아니긴 하지만말이다. Unbiasedness를 알아가면서 우리는 최적의 estimator를 골라내기 위해서는 어떤 성질들을 고려해야 하고, 또 unbiased estimator 자체에 어떤 의미가 있는지도 함께 알아볼 수 있다는 점에 큰 의미가 있다. 그럼으로써 최적의 estimator을 찾기 위해 신경써야 하는 부분들을 직관적으로 알고 이를 큰 틀에서 바라볼 수 있다.

<br>
<br>

# Minimum Variance Unbiased Estimators <sub>최소분산불편추정량</sub>

먼저 estimator가 unbiased하다는 것이 어떤 의미인지 수학적으로 알아보자. $g(\theta)$의 estimator $\delta$가 *unbiased*<sub>불편향된</sub>하다는 것은 

$$ E_{\theta} \delta(X) = g(\theta), \quad\forall\theta\in\Omega $$

라는 의미이다. 만약 실제로 unbiased estimator $\delta$가 존재한다면, *$g$는 U-estimable*<sub>??</sub>하다고 말한다. 예를 살펴보자. 

<br>

> **Example 10.1** Random variable $X$가 $\mathcal{U}(0, \theta)$를 따른다고 하자. $g(\theta)$의 estimator $\delta$가 unbiased하기 위해서는 다음을 만족하면 된다.
>
> $$ E\delta(X) = \int_0^{\theta} \delta(x)\cdot \frac{1}{\theta}\,dx = g(\theta), \quad\forall\theta >0 $$
>
> 정리하면
>
> $$\int_0^{\theta} \delta(x)\,dx = \theta g(\theta), \quad \forall\theta >0 $$
>
> 그러므로 $\theta\to0$일 때 $\theta g(\theta)\to 0$이 되지 않으면 $g$는 U-estimable하지 않을 것이다. 만약, $g$가 differentiable하다면, FTC(fundamental theorem of calculus)에 의해 양변을 미분하여 우리는 다음을 얻을 수 있다.
>
> $$ \delta(x) = g(x) + xg'(x) $$
>
> 만약 $g(\theta) = \theta$라면 $\delta(X) = 2X$는 unbiased estimator인 것이다.
>

<br>

만약에 loss function으로 squared loss $L(\theta, d) = (d - g(\theta))^2$를 활용한다면 unbiased estimator의 risk function은

$$ R(\theta, d) = E_{\theta}(\delta(X) - g(\theta))^2 = Var_{\theta}(\delta(X))$$ 

그러므로 unbiased estimator의 risk를 최소화하는 것은 결국 unbiased estimator의 variance를 최소화하는 것과 같다. 그리고 variance를 최소화시킨다면, 이는 결국 unbiased estimator 중에서는 월등하다고 볼 수 있다. 그런 의미에서 우리는 variance를 최소화시킨 unbiased estimator에게 특별한 이름을 붙여준다.

<br>

> **Definition 10.2** Unbiased estimator $\delta$가 *uniformly minimum variance unbiased(UMVU)*<sub>전역최소분산불편추정량</sub>하다는 것은, 그 어느 다른 unbiased estimator $\delta^*$에 대하여
>
> $$ Var_{\theta}(\delta)\le Var_{\theta}(\delta^*), \quad\theta\in\Omega$$
>
> 를 만족하는 것이다. 
>

<br>

그렇지만 위의 정의로는 주어진 unbiased estimator가 UMVU estimator(UMVUE)인지 아닌지 판단하기 굉장히 어려울 것이다. 모든 unbiased estimator와 variance를 비교하는 것은 말도 안 되기 때문이다. $g(\theta)$의 어떤 unbiased estimator $\delta = \delta(X)$를 고려해보자. 또다른 unbiased estimator를 고려하기 위해 모든 $\theta\in\Omega$에 대해 $E_{\theta}U = 0$(즉, $U$는 0의 unbiased estimator)인 $U$와 constant $a$에 대하여

$$ \delta(X) + a\cdot U(X)$$

가 더 좋은 performance를 가질 조건을 탐색해보자. 두 unbiased estimator의 performance 비교는 곧 variance 비교임을 위에서 확인한 바 있다.

$$ Var_{\theta}(\delta+aU) =Var_{\theta}(\delta) + 2aCov_{\theta}(\delta, U) + a^2 Var_{\theta}(U)$$

만약 어떤 $\theta\_0\in\Omega$에 대하여 $Cov\_{\theta}(\delta, U)>0$라고 하자. 그러면 $a$를 적절히

$$ \frac{-2Cov_{\theta_0}(\delta, U)}{Var_{\theta_0}(U)}\lt a\lt 0$$

를 만족하도록 잡으면 $2aCov\_{\theta_0}(\delta, U) + a^2Var\_{\theta_0}(U)\lt 0$이므로 $\delta$는 UMVUE가 될 수 없다. 이는 어떤 $\theta\_0\in\Omega$에 대하여 $Cov\_{\theta}(\delta, U)<0$인 경우에도 마찬가지이다. 그러므로 우리는 여기서 $\delta$가 UMVUE이면, 모든 0의 unbiased estimator $U$에 대해 $Cov\_{\theta}(\delta, U)=0,\,\forall\theta$임을 알 수 있다.

놀랍게도 우리가 찾은 조건은 단지 sufficient condition이 아니라 equivalent한 condition이다. 다음 theorem을 살펴보자.

<br>

> **Theorem 10.3** $g(\theta)$의 unbiased estimator $\delta$가 UMVUE가 되는 것은 모든 0의 unbiased estimator와 $\delta$가 uncorrelated되었다는 것과 equivalent하다.
>
> *Proof*: $\delta$가 UMVUE이면, 모든 0의 unbiased estimator와 $\delta$가 uncorrelated되어 있음을 위에서 이미 증명하였다.
>
> 이제 converse<sub>역</sub>을 증명하자. 모든 0의 unbiased estimator와 $\delta$의 correlation이 없음이 가정되었다. $g(\theta)$의 또다른 unbiased estimator $\eta$에 대하여 $\eta - \delta$는 0의 unbiased estimator임을 알 수 있다. 따라서
>
> $$
> \begin{align*}
> Var_{\theta}(\eta) &= Var_{\theta}(\delta + (\eta - \delta)) \\
> &= Var_{\theta}(\delta) + Var_{\theta}(\eta-\delta) \\
> &\ge Var_{\theta}(\delta)
> \end{align*}
> $$
>
> $\eta$는 임의로 잡은 unbiased estimator였으므로 우리는 $\delta$가 UMVUE임을 알 수 있다.
> 

<br>

이 theorem은 결국 UMVU estimator가 하나라도 존재만 한다면 uniqueness<sub>유일성</sub>을 가지고 있음을 알려주기도 한다. 정말 주요한 사실이 유도되는 것으로 보아 위의 equivalent한 condition은 큰 역할을 담당하고 있다.

<br>

> **Theorem 10.4** 만약 $\delta$가 $g(\theta)$의 UMVUE라면, $\delta$는 unique하다. 즉, 만약 $\delta^*$이 또다른 $g(\theta)$의 UMVUE이면, 
>
> $$ P_{\theta} (\delta(X) = \delta^*(X)) =1, \quad\forall\theta\in\Omega$$
>
> 라는 것이다.
>
> *Proof*: $\delta$와 $\delta^\*$가 모두 $g(\theta)$의 UMVUE라고 가정하자. 둘 다 unbiased이기 때문에 $\delta - \delta^\*$은 0의 unbiased estimator이다. **Theorem 10.3**에 의하여
>
> $$ Cov_{\theta} (\delta, \delta-\delta^*) =0, \quad Cov_{\theta}(\delta^*, \delta-\delta^*)=0$$
>
> 이다. 그러므로
>
> $$
> \begin{align*}
> E_{\theta}(\delta-\delta^*)^2 &= E_{\theta}\delta(\delta-\delta^*) - E_{\theta}\delta^*(\delta-\delta^*) \\
> &= Cov_{\theta}(\delta, \delta-\delta^*) - Cov_{\theta}(\delta^*, \delta-\delta^*) \\
> &=0
> \end{align*}
> $$
>
> 따라서 $P_{\theta}(\delta - \delta^\* = 0) = 1$임이 증명되었다.

<br>

좋은 소식은 만약 complete sufficient statistic이 있다면, $g$가 U-estimable하다면야 UMVUE가 존재함이 알려져 있고, 역시 유일하다는 것이 증명되어 있다. 심지어 그 유일한 UMVU estimator가 complete sufficient statistic에 대하여 어떠한 형태로 나타나게 되는지도 증명에서 제시해준다.

<br>

> **Theorem 10.5 (Lehmann-Scheffe)** $g$가 U-estimable하고 $T$가 complete sufficient하다고 하자. 그러면 $T$를 base로 하는 unbiased estimator는 unique하며, 이는 UMVUE이다. 
>
> *Proof*: $\delta =\delta(X)$가 unbiased estimator라고 하고 다음을 정의하자.
>
> $$ \eta(T) = E[\delta\vert T] $$
>
> 이 형태는 Rao-Blackwell theorem에서 많이 본 형태이기도 하다. Smoothing에 의하여
>
> $$ E_{\theta} \eta(T) = E_{\theta}E_{\theta}[\delta\vert T] = E_{\theta}\delta = g(\theta)$$
>
> 이므로 $\eta(T)$는 unbiased estimator이다. 먼저, 우리는 $T$로 표현할 수 있는 unbiased estimator는 $\eta$가 유일함을 증명할 것이다. $\eta^\*(T)$ 또한 unbiased라고 가정하자. 그러면,
>
> $$E_{\theta}[\eta(T) - \eta^*(T)]=0, \quad\forall\theta\in\Omega $$
>
> $T$의 completeness에 의해 $\eta(T) - \eta^*(T) = 0$ (a.e. $\mathcal{P}$)가 성힙한다. 이는 다시 말해 $\eta$가 유일함을 보여준다. 그 유일한 estimator의 형태가 unbiased estimator에 대하여 $E_{\theta}[\delta\vert T]$의 로 알려져 있으니 이는 의미가 매우 크다.
>
> $\eta(T)$는 Rao-Blackwell theorem에 의해 convex loss(특히 여기서는 squared loss) 아래에서는 $\eta(T)$가 최소의 variance를 가지고 있을 것이다. 다르게 표현하면, 어떤 다른 unbiased estimator $\delta^\*$에 대하여 $\eta^\*(T) = E_{\theta}(\delta^\*\vert T)$를 잡자. $\eta^\*$ 역시 unbiased 이다. Squared loss 하에서는 모든 unbiased estimator의 risk function이 variance 이므로
>
> $$Var_{\theta}(\delta^*)\ge Var_{\theta}(\eta^*(T)) = Var_{\theta}(\eta(T)), \quad\forall\theta\in\Omega$$
>
> 따라서 $\eta(T)$는 UMVU하다.
>

<br>

위의 theorem이 말해주는 것은, 어쩌다 찾은 unbiased estimator가 complete하고 sufficient한 $T$의 함수로 표현 가능하다면, 그 찾은 estimator는 무조건 UMVUE가 되는 것이다. 이 theorem은 다음 두 가지를 말해준다.

1. 우리가 찾은 unbiased estimator가 하~~필 $T$에 관한 식으로 표현된다면, 그 unbiased estimator는 UMVUE이다. 
2. 혹은 우리가 고의로 unbiased estimator를 $T$에 관한 식으로 만들어버릴 수도 있다. 아무 unbiased estimator $\delta$를 찾으면 $\eta(T) = E[\delta\vert T]$를 계산하여 UMVUE $\eta$를 얻을 수 있다.

먼저 1번에 대한 예시를 살펴보자.

<br>

> **Example 10.6** $X_1, \cdots X_n$이 $(0, \theta)$ 위에서의 uniform distribution에서 추출된 i.i.d. random sample이라고 하자. 이 data의 joint density는
>
> $$p_{\theta}(x) = \frac{1}{\theta^n}\cdot 1_{(-\infty, \theta)}(\max X_i) $$
>
> 따라서 factorization theorem에 의해 $T = \max_i X_i$는 sufficient하다. 또, $p_{\theta}(x)\propto_{\theta} p_{\theta}(y)$를 가정하면 $\max X_i = \max Y_i$이므로 $T(x) = T(y)$이다. 그러므로 $T$는 complete이기도 함을 알 수 있다. 
>
> 이제 $\eta(T)$가 $g(\theta)$의 unbiased estimator라고 하자. 그러면
>
> $$\int_0^{\theta} \eta(t) \frac{nt^{n-1}}{\theta^n}\,dt = g(\theta), \quad \theta\gt 0$$
>
> 이는 다시 말해
>
> $$ n\int_0^{\theta} t^{n-1}\eta(t)\,dt = \theta^n g(\theta), \quad \theta\gt 0$$
>
> 라는 의미다. 만약 $g$가 differentiable하다면, 양변을 $\theta$로 미분하여
>
> $$n\theta^{n-1}\eta(\theta) = \frac{d}{d\theta}(\theta^n g(\theta)) $$
>
> 를 얻고, 더 정리해보면
>
> $$\eta(t) = g(t) + \frac{tg'(t)}{n}, \quad t >0$$
>
> 와 같다. 만약 $g(\theta) = \theta$라면, $\eta(t) = (n+1)t/n$를 얻는다. 그러므로
>
> $$ \eta(T) = \frac{(n+1)T}{n}$$
>
> 는 $\theta$의 UMVUE이다. 이렇게 찾은 $\eta(T)$가 실제로 성능이 좋은지 간단하게 비교해보자. $\mathcal{U}(0, \theta)$에서 추출된 한 random variable의 expectation이 $\theta/2$라는 점을 이용하면 $\delta = 2\overline{X}$는 $\theta$의 unbiased estimator임을 알 수 있다.
>
> $\eta(T)$의 risk를 구하기 위해 먼저 $T$의 density를 조사해야 한다. $T$의 CDF는 다음과 같이 계산할 수 있다.
>
> $$F_T(t) = P_{\theta}(X_1\le t, \cdots, X_n\le t) = P_{\theta}(X_1\le t)\times\cdots\times P_{\theta}(X_n\le t) = \left(\frac{t}{\theta}\right)^n, \quad 0<t<\theta $$
>
> 그러므로 $T$의 density는
>
> $$f_T(t) = \frac{nt^{n-1}}{\theta^n}, \quad 0<t<\theta $$
>
> 이제 $\eta(T)$의 risk를 구해보자. $\eta$는 unbiased하기 때문에 risk가 곧 variance이다.
>
> $$
> \begin{align*}
> E_{\theta} \eta^2(T) &= \left(\frac{n+1}{n}\right)^2E_{\theta}T^2 = \left(\frac{n+1}{n}\right)^2\int_0^\theta t^2 \frac{nt^{n-1}}{\theta^n}\,dt = \frac{(n+1)^2}{n(n+2)}\theta^2 \\
> Var_{\theta}(\eta(T)) &= E_{\theta}\eta^2(T) - (E_{\theta}\eta(T))^2 = \frac{(n+1)^2}{n(n+2)}\theta^2 - \theta^2 = \frac{\theta^2}{n(n+2)} 
> \end{align*}
> $$
>
> 이제 $\delta$의 risk를 구해보자. $\delta$ 역시 unbiased하므로 risk는 곧 variance이다. 
>
> $$
> \begin{align*}
> Var_{\theta}(X_1) &= EX_1^2 - (EX_1)^2 = \frac{\theta^2}{3} - \frac{\theta^2}{4}=\frac{\theta^2}{12}\\
> Var_{\theta}(\delta) &= \frac{4}{n^2}\sum_i Var_{\theta}(X_i) = \frac{\theta^2}{3n} 
> \end{align*}
> $$
>
> $\eta$와 $\delta$의 risk 비율은 따라서
>
> $$\frac{R(\theta, \eta)}{R(\theta, \delta)} = \frac{Var_{\theta}(\eta(T))}{Var_{\theta}(\delta)} = \frac{3}{n+2} $$
>
> $n\to\infty$ 이면 두 risk의 비율은 0으로 향한다. 따라서 risk의 관점에서 $\eta(T)$가 훨씬 더 정확한 estimator임을 알 수 있고, 이 성질은 sample size $n$이 커질 수록 더 두드러진다.
>

<br>

한 번에 UMVUE를 찾다니, 이는 보통 운이 아닐 수 있다. 일반적으로 UMVUE가 한 번에 찾아지지 않는 경우, 우리는 위의 2번 과정을 이용해도 된다. **Example 10.6**에서 $\eta$를 아직 못 찾았다고 해보고 간단히 찾은 unbiased estimator $\delta$를 이용하여 찾는 과정을 다음 예시에서 살펴보자.

<br>

> **Example 10.7** 우리는 **Example 10.6**와는 다르게 $\eta(T)$를 한 번에 찾지 못했다고 가정하고, 간단하게 찾은 $\delta = 2\overline{X}$를 이용하여 UMVUE를 구할 것이다. 우리는 **Theorem 10.5**에 의해 다음을 계산해야 한다.
>
> $$ \eta(T) = E[2\overline{X}\vert T] = 2E[\overline{X}\vert X_{(n)}] $$
>
> 먼저 $T = X_{(n)}$는 complete sufficient하고, $V = \overline{X}/X_{(n)}$는 ancillary하다. $V$가 ancillary하다는 점은 $Y_i \overset{\text{i.i.d.}}{\sim} \mathcal{U}(0, 1)$에서 $\overline{X}/X_{(n)} = \overline{X}/X_{(n)}$으로써 $\theta$와 무관함을 알 수 있기 때문이다. 그러므로 Basu 정리에 의하여 $T$와 $V$는 independent하다. 그러면 우리는 $\eta$를 다음과 같이 간편화할 수 있다.
>
> $$
> \begin{align*}
> \eta(T) &= 2X_{(n)}E\left[\frac{\overline{X}}{X_{(n)}} \bigg\vert X_{(n)}\right] \\
> &= 2X_{(n)}E\frac{\overline{X}}{X_{(n)}}\\
> &= \frac{2}{n}X_{(n)}\sum_i E\frac{X_i}{X_{(n)}} \\
> &= \frac{2}{n}X_{(n)}\sum_i E\frac{X_{(i)}}{X_{(n)}}
> \end{align*}
> $$
>
> 그러므로 우리가 마지막으로 하면 되는 것은 $X\_{(i)}/X\_{(n)}$의 distribution만 알면 된다. 먼저 두 order statistics $(X_{(i)}, X_{(n)})$의 joint density는 
>
> $$f_{X_{(i)}, X_{(n)}}(x_i, x_n) = \frac{n!}{(i-1)!(n-i-1)!}\left(\frac{x_i}{\theta}\right)^{i-1}\frac{1}{\theta}\left(\frac{x_n-x_i}{\theta}\right)^{n-i-1}\frac{1}{\theta}, \quad 0<x_i\le x_n <\theta$$
>
> 이다. 이제 transformation $(X\_{(i)}, X\_{(n)})\mapsto (X\_{(i)}/X\_{(n)}, X_{(n)})$을 적용하면
>
> $$
> \begin{align*}
> f_{V, T}(v, t) &= f_{X_{(i)}, X_{(n)}}(x_i, x_n)\cdot\bigg\vert\frac{\partial(x_i, x_n)}{\partial(v, t)}\bigg\vert \\
> &=  \frac{n!}{(i-1)!(n-i-1)!}\frac{1}{\theta^n}(vt)^{i-1}(t-vt)^{n-i-1}\cdot t \\
> &=  \frac{n!}{(i-1)!(n-i-1)!}\frac{1}{\theta^n}v^{i-1}(1-v)^{n-i-1}\cdot t^{n-1}, \quad 0<v<1, 0<t<\theta
> \end{align*}
> $$
>
> 가 도출된다. 참고삼아 남겨두지만 이렇게 pdf가 두 변수에 대한 각각의 함수로 쪼개지면(domain까지 indicator로 표현했을 때) $V$와 $T$는 independent하고 marginal pdf를 분리해서 가져가면 된다. 혹은  다른 관점으로 $n=1$일 때의 complete sufficient statistic과 ancillary statistic의 관계이기에 independent하다고 생각해도 무방하다. 무튼 $V$의 marginal density는
>
> $$ f_V(v) \propto v^{i-1}(1-v)^{n-i-1}$$
>
> 이므로 $V\sim Beta(i, n-i)$이다. 이 distribution의 expectation은 $i/n$이므로
>
> $$\eta(T) = \frac{2}{n}X_{(n)}\cdot \frac{n(n+1)/2}{n} = \frac{n+1}{n}X_{(n)}$$
>
> 이다. 결과가 **Example 10.6**과 동일함을 관찰하라.
>

<br>
<br>

# Bias<sub>편향</sub>

지금까지 우리는 frequentist의 입장을 따라 unbiased estimator가 가장 합리적일 것이라는 전제에 따라 그 안에서만 가장 좋은 estimator을 찾는 과정을 거쳤다. 맨 처음에 언급했듯, unbiased estimator가 bias를 0으로 만들어주었기에 가장 합리적일 것이라고 생각했는데 이번에는 꼭 모든 상황에서 그렇지 않음을 언급하려고 한다. 물론 아직 unbiased estimator에 관한 중요한 사실을 모두 언급하진 않았지만, 그 점을 더 알아보기 전에 estimator에 대한 큰 틀을 정립하는 것이 도움될 것이다. 그러면 이후에도 unbiased estimator의 특징들과 다른 estimator들에 대해서도 조금 더 입체적으로 이해할 수 있을 것이다. 어떠한 점이 unbiased estimator가 항상 답이 아니라는 것을 나타내주는지 차근차근 살펴보자.

<br>

> **Example 10.8** 우리는 이전 예시에서 $\mathcal{U}(0, \theta)$로부터 추출된 random sample $X = (X\_1\ \cdots X\_n)^T$에 대하여 $T = max X_i = X_{(n)}$가 complete sufficient statistic임을 알았고, $g(\theta) = \theta$의 UMVUE는 $\eta(T) = (n+1)T/n$임을 두 가지 방법으로 도출하였다. 하지만, 여기에서는 다시 묻고자 한다. 굳이 unbiased에 집착하지 않는다고 할 때, $T$를 기반으로 만들 수 있는 estimator 증에서 $\eta(T)$가 그럼에도 최선인가?
>
> 뭔가 아닌 것 같으니, 더 좋은 estimator를 간단하게 찾아보자. 어떤 상수 $a$에 대하여 $\eta$보다 더 좋은 estimator가 $\delta_a = aT$의 꼴을 가진다고 가정하자. 이제 $\delta_a$의 risk를 squared loss 아래에서 계산해볼 것인데, $\delta_a$는 unbiased하지 않으므로 risk가 variance하고 정확히 일치하진 않는다. 우린 **Example 10.6**에서 $T$의 density를 계산했고, 이를 이용하면
>
> $$ E_{\theta}T = \frac{n\theta}{n+1}, \quad E_{\theta}T^2 = \frac{n\theta^2}{n+2} $$
>
> 이다. 그러므로 $\delta_a$의 risk는
>
> $$
> \begin{align*}
> R(\theta, \delta_a) &= E_{\theta}(aT - \theta)^2 \\
> &= a^2 E_{\theta}T^2 - 2a\theta E_{\theta}T + \theta^2 \\
> &= \left(\frac{n}{n+2}a^2 - \frac{2n}{n+1}a +1 \right)
> \end{align*}
> $$
>
> 이다. 이 risk는 $a_0 = (n+2)/(n+1)$일 때 optimize(즉, 여기선 minimize)된다. 이때의 $\delta\_{a\_0}$의 risk와 $\eta$의 risk를 비교해보자.
>
> $$
> \begin{align*}
> R(\theta, \delta_{a_0}) &= \frac{\theta^2}{(n+1)^2} \\
> R(\theta, \eta) &= \frac{\theta^2}{n(n+2)}
> \end{align*}
> $$
>
> $\delta\_{a\_0}$의 risk가 조금 더 작음을 알 수 있다. 
>

<br>

Unbiased를 포기했더니 unbiased estimator 중에서 제일 갑이라는 UMVUE보다도 더 performance가 좋은 estimator를 **Example 10.8**에서 찾을 수 있었다. 어떻게 가능했을까?

Squared loss를 전제했을 때, $g(\theta)$의 임의의 estimator $\delta$에 대하여 risk function은 다음과 같이 분해될 수 있다. 수학적으로는 간단하게 증명 가능하다.

$$ R(\theta, \delta) = E_{\theta}(\delta - g(\theta))^2 = Var_{\theta}(\delta)+b^2(\theta, \delta)$$

이때, $b(\theta, \delta)$는 true value와 estimator의 expectation 간의 차이로, *bias*라고 부른다. 이것이 0일 때 우리는 해당 estimator을 unbiased estimator라고 부른 것이다. 수학적으로 표현하면

$$ b(\theta, \delta) = E_{\theta}\delta - g(\theta )$$

으로 표현된다. 위에서 risk function을 bias와 variance로 분해한 것에 대해서 조금 더 풀어서 살펴보자.

1. **Bias**: 지금까지는 0으로 놓는 것이 가장 깔끔하다고 주장하던 개념이다. 기왕 estimate하는데 평균적으로 true value로 예측하지 않는 다는 것은 조금 이상하게 들릴 수 있기 때문이다. 만약 bias가 positive이면 그 estimator는 true value보다 조금 크게 예측하려는 경향이 있는 것이다. 반대로 bias가 negative이면 그 estimator는 true value보다 조금 작게 예측하려는 겅향이 있다고 볼 수 있다.
2. **Variance**: Estimator의 variance. Estimator도 기본적으로 data sample에 기반하기 때문에 random 요소가 있다. 조금 더 풀어 설명하면, sample이 어떻게 선택되냐에 따라 estimator의 값도 달라진다는 이야기다. 만약 estimator의 variance가 크다면, sample을 뽑을 때마다 그 estimator를 통해 도출되는 estimated 값들이 넓게 분포되어 있다는 것이다. 반대로 estimator의 variance가 작다면, sample을 뽑을 때마다 도출되는 estimated 값이 좁게 분포되어 한 부분에 몰려있다는 뜻이다.

이제 두 개념이 직관적으로 이해가 되었을 것이다. 사실 가장 이상적인 것은 bias도 최대한 줄이고 variance를 최대한 줄이면 된다. 하지만 **Example 10.8**에서는 무슨 일이 일어났길래 bias를 조금 늘렸더니 오히려 더 작은 risk를 가진 estimator를 얻었을 수 있던 것일까? UMVUE $\eta$와 위에서 찾은 biased estimator $\delta\_{a\_0}$를 bias와 variance로 다시 비교해보자.

<br>

|estimator|bias|variance|risk|
|---|---|---|---|
|$\eta$|$0$|$\displaystyle \frac{\theta^2}{n(n+2)}$|$\displaystyle \frac{\theta^2}{n(n+2)}$|
|$\delta\_{a\_0}$|$\displaystyle -\frac{\theta}{(n+1)^2}$|$\displaystyle \frac{n(n+2)\theta^2}{(n+1)^4}$|$\displaystyle \frac{\theta^2}{(n+1)^2}$|

<br>

먼저 bias를 비교해보면 역시 $\delta\_{a\_0}$가 더 큰 절댓값 bias를 가지고 있다. Unbiased 성질을 포기한 모습을 볼 수 있다. Variance를 비교해보면 오히려 $\delta\_{a\_0}$가 $\eta$보다 작음을 알 수 있다. 때문에 $\delta\_{a\_0}$는 bias 때문에 risk가 자칫 증가할 수 있었음에도 늘어난 bias보다 더 큰 폭으로 variance를 줄였기 때문에 궁극적으로 risk가 $\eta$보가 줄어들었다.

이러한 현상을 *bias & variance trade-off* 관계라고 자주 언급한다. 즉, bias와 variance는 서로 trade-off<sub>상호베타적인</sub>인 관계에 놓여있는 경우가 꽤나 자주 있다는 것이다. 그러므로 bias와 variance를 둘 다 최대한 줄여서 만든 estimator의 risk보다, bias나 variance 둘 중 하나를 적당히 포기하고 하나를 더 최소화시켜 만든 estimator의 risk가 더 효과적일 수 있다는 것이다. 그러므로 estimator를 unbiased인 것에 대해서만 살펴보기엔 가장 좋은 performance를 가진 estimator를 못 찾을 수 있다. 이를 직관적으로도 한 번 받아들여보면, 아무리 bias가 0이어서 true value에 평균적으로 맞게 예측을 해도 variance가 아직 너무 커 평균만 true value에 맞추었을 뿐, sample에 따라 estimated 값이 여기저기 튀어버린다면 좋은 estimate를 할 수 없을 것이다. 따라서 bias와 variance를 조절하는 것은 아주 중요하며, 어느 model을 개발하든 이 문제에는 항상 직면하게 된다. Frequentist의 초점은 결국 *sample을 반복적으로 뽑는다*에 맞춰져 있기 때문에 sample를 반복해서 추출했을 때 우리가 만든 수치, 모델 등등은 어떤 일이 일어나는지 머릿속에서 상상해보는 것은 직관적으로 이해하는데 도움을 준다.

Unbiasedness가 결국은 expectation 값만을 설명하고 실질적인 estimate performance를 설명하기엔 부족한 면이 있음을 이해했을 것이다. 다음은 이에 대한 극단적인 케이스로 아주 유명한 예시이다.

<br>

> **Example 10.9** Random variable $X$가 다음과 같은 mass function을 가지고 있다고 하자.
>
> $$P_{\theta}(X = x) = \frac{\theta^x \cdot e^{-\theta}}{x!(1-e^{-\theta})}, \quad x = 1, 2, \cdots $$
>
> 복잡해보이지만 사실은 Poisson distribution을 $\\{1, 2, \cdots\\}$위로 truncate한 distribution의 mass이다. 이 density의 꼴을 보아 exponential family에 속하고 $X$는 complete sufficient함을 쉽게 알 수 있다(증명해보아라).
>
> 우리는 $g(\theta) = e^{-\theta}$를 예측하고자 한다. $g(\theta)$는 Poisson distribution을 truncate하여 버린 정보의 비율, 혹은 다르게 표현하면 원래 Poisson distribution에서 전체 중 0인 만큼의 비율이다. 발생하는 rate가 $\theta$인 사고가 각 사람마다 발생한 수를 data를 얻고, truncated Poisson distribution을 이를 위한 model이라고 생각하면, $g(\theta)$는 무사고율을 나타냄을 알 수 있다. 즉, 무사고율을 예측하기 위한 합당한 estimator를 만들고자 하는 것이다. Model이 정상적인 Poisson distribution을 냅두고 $\\{1, 2, \cdots\\}$로 truncate된 Poisson distribution을 사용하는 이유는 0이라는 값이 관측이 불가능한 data가 있기 때문이고 이 상황도 마찬가지이다. 사고의 건수가 0인 사람의 data는 기본적으로 data 목록에 없을 것이라고 생각할 수 있다. 이는 사고가 없는 것이 따로 보고되지 않기 때문이다. 사고가 한 번 이상 발생하면 그때서야 어떤 사람이 사고를 당했는지 보고되므로 해당 사람의 사고 발생 수가 1이 되는 것이지, 그 사고가 발생하기 전까지는 그 사람은 data 목록조차에 있지도 않았을 것이다. Trunacted Poisson distribution을 활용하는 또 다른 상황의 예로는 입원일 수(최소 기간이 1일, 0일 입원은 입원한 것이 아니므로), 공장에서의 각 노동자 사고 수, 각 가구당 전염병 감염 수 등이 있다.
>
> 다시 우리의 이야기로 돌아와서, 만약 $\delta(X)$가 unbiased라면 정의에 의해 다음을 만족해야 한다.
>
> $$ E_{\theta}\delta(X) = \sum_{k=1}^\infty \delta(k)\cdot\frac{\theta^k e^{-\theta}}{k!(1-e^{-\theta})} = e^{-\theta}, \quad \theta >0 $$
>
> 그러므로
>
> $$ \sum_{k=1}^\infty \frac{\delta(k)}{k!}\theta^k = 1-e^{-\theta} = \sum_{k=1}^\infty\frac{(-1)^{k+1}}{k!}\theta^k, \quad \theta >0 $$
>
> Power series expansion<sub>다항식전개</sub>은 uniqueness가 있음이 알려져 있다. 그러므로 두 power series가 같을 조건은 각 $\theta^k$의 coefficient<sub>계수</sub>가 모두 같아야 한다. 따라서
>
> $$ \delta(k) = (-1)^{k+1}$$
>
> 이다. $T = X$가 complete sufficient임에 따라서
>
> $$\delta(X) = (-1)^{X+1}$$
>
> 가 UMVUE이다. 즉, data $X$가 odd<sub>홀수</sub>면 1이고 even<sub>짝수</sub>면 -1이라는 estimate 값이 나온다. 무사고율 예측은 기본적으로 0과 1사이의 값이 나와야 해석가능해서 1과 -1 둘 다 해석 불가한 값이다. 즉, 만약에 어떤 사람이 652번의 사고를 당한 기록이 있으면, 앞으로의 무사고율이 1이라는 말도 안 되는 결과와, 만약에 그 사람이 최근에 한 번의 사고를 더 당해서 603번의 사고 기록이 있으면, 앞으로의 무사고율이 -1이라는 최악으로 해석이 안 되는 결과가 도출된다.
>
> 하지만 놀랍게도 이 estimator의 값은 data를 계속 뽑을 수록 평균적으로 아무튼 $e^{-\theta}$의 true value가 나온다는 것이다. Unbiased estimator가 항상 정답이 아님을 보여주는 단적인 예이다.
>

<br>
<br>

---
다음 post에서는 unbiased estimator의 risk(즉, variance)에 대한 lower bound를 제공하는 매우 유명한 inequality를 알아볼 것이다. 이때 중요한 수치 하나가 소개되는데, 관측 가능한 data $X$가 평균적으로 parameter $\theta$의 information을 얼마나 담고 있는지, $X$를 통해 $\theta$을 추정할 수 있는 경향이 좋은지에 대한 값으로 앞으로의 inference에서 굉장히 많이 활용된다.