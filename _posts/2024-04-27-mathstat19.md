---
title: Large Sample Theory - Convergence
date: 2024-04-27 00:54:00 +09:00
categories: [Statistics, Mathematical Statistics]
math: true
pin: true
tags:
  [
    probability,
    mathematical statistics,
    large sample,
    converge in probability,
    converge in distribution,
    consistency,
    approximation,
    large sample theory
  ]
---


이번 편에서는 다시 원래의 맥락으로 돌아가 large sample theory를 다룰 것이다. 이 내용 역시 frequentist의 심장같은 철학을 수학적으로 나타내는 도구라고 볼 수 있다. 이들이 probability를 정의할 때 infinte sequence of experiment에서 어떤 특정 event의 frequency로 정의했으므로 experiment를 무한 번 진행했을 때 어떻게 되는지, 다시 말하여 sample의 개수가 infinte할 때 어떻게 되는지가 매우 중요한 것이다. 그리고 이들의 논리가 맞춰지기 위해서는 sample size를 무한이 늘렸을 때 어떤 estimate 값이 true value로 다가가야 한다. 이와 관련된 성질이 *consistency*<sub>일치성</sub>이며, 위와 같은 이유로 frequentist는 estimator가 consistency를 만족하는지의 여부를 굉장히 중요시한다. 그러므로 large sample theory는 estimator가 과연 frequentist의 핵심 가치인 consistency를 만족하는지, 혹은 infinie size일 때 estimator가 따르는 distribution이 무엇일지 파악하여 근사하는 방법론을 제시해주는 것이 목적이 되겠다. 또한, estimator 각각을 sample size를 infinte하게 키웠을 때 여전히 존재하는 variance 등을 비교하면서 performance를 다시금 비교해볼 것이다. Large sample theory는 몇 편에 나누어서 연재되는데, 다양한 convergence와 limit theorem 등을 다루고, 실제 estimator들의 performance를 비교하는 metric을 다룰 예정이다.

<br>
<br>
<br>

# Convergence in Probability

Random variable이 converge한다는 것은 그 값이 어느 정도 범위 내에 모이는 probability가 굉장히 높아졌을 때를 의미한다.

<br>

> **Definition 19.1** Sequence of random variables $Y_n$이 $n\to\infty$에 따라 random variable $Y$로 *converge in probability*한다는 것은, 모든 $\epsilon\gt0$에 대하여 $n\to\infty$이면
>
> $$P(\vert Y_n - Y\vert \ge\epsilon)\to 0$$
>
> 이라는 것이다. 이때 표기는
>
> $$ Y_n\overset{p}{\to} Y$$
>
> 라고 한다.

<br>

가장 기본적인 property부터 알아보자.

<br>

> **Theorem 19.2 (Chebyshev's Inequality)** 임의의 random variable $X$와 임의의 constant $a>0$에 대하여
>
> $$ P(\vert X\vert \ge a) \le \frac{EX^2}{a^2}$$
>
> 이 성립한다.
>
> *Proof:* 임의의 $X$에 대하여 다음이 성립함을 알 수 있다.
>
> $$ I\{\vert X\vert \ge a\} \le \frac{X^2}{a^2} $$
>
> 이제 위에 expectation만 씌우면 결론이 도출된다.

<br>

> **Proposition 19.3** $n\to\infty$임에 따라 $E(Y\_n - Y_^2)\to0$이면, $Y\_n\overset{p}{\to}Y$이다.
>
> *Proof:*  Chebyshev's inequality(**Theorem 19.2**)에 의하여 임의의 $\epsilon>0$에 대해
>
> $$P(\vert Y_n - Y\vert \ge\epsilon) \le \frac{E(Y_n - Y)^2}{\epsilon^2}\to 0 $$
>
> 이므로 증명되었다.

<br>

이를 활용하여 sample mean이 꽤나 주요한 property를 가지고 있음을 알 수 있다.

<br>

> **Example 19.4** $X\_1, \cdots, X\_n$이 mean은 $\mu$이고 variance는 $\sigma^2$인 population으로부터 i.i.d. 하게 추출된 sample이라고 하고, 이때의 sample mean을 $\overline{X}\_n = (X\_1+\cdots+X\_n)/n$이라고 하자. 그러면
>
> $$E(\overline{X}_n - \mu)^2  = Var(\overline{X}_n) = \sigma^2/n \to 0$$
>
> 이므로 $n\to\infty$임에 따라 $\overline{X}\_n\overset{p}{\to}\mu$를 만족한다. 
>
> 이는 심지어 $\sigma = \infty$여도 $E\vert X\_i\vert\lt\infty$이면 성립함이 알려져 있다. 이 주요한 성질을 *weak law of large numbers*<sub>약한 큰 수의 법칙</sub>이라고 한다.

<br>

다음 성질은 매우 요긴하게 활용할 것이다.

<br>

> **Proposition 19.5** 만약 $f$가 $c$에서 continuous하고 $Y\_n\overset{p}{\to}c$라고 할 때, $f(Y\_n)\overset{p}{\to}f(c)$라고 한다.
>
> *Proof*: $f$가 $c$에서 continuous하기 때문에, 임의의 $\epsilon>0$에 대하여 다음을 만족하는 $\delta\_{\epsilon}$이 존재한다.
>
> $$ \vert y - c\vert\lt\delta_\epsilon\Rightarrow\vert f(y) - f(c)\vert\lt\epsilon$$
>
> 그러므로
>
> $$
> \begin{align*}
> \{e: \vert Y_n(e) - c\vert\lt \delta_{\epsilon} \}&\subset \{e: \vert f(Y_n(e)) - f(c)\vert \lt \epsilon\} \\
> P(\vert Y_n - c\vert\lt\delta_{\epsilon})&\le P(\vert f(Y_n) - f(c)\vert \lt\epsilon) \\
> P(\vert f(Y_n) - f(c)\vert \ge\epsilon)&\le P(\vert Y_n-c\vert \ge\delta_{\epsilon})\to 0
> \end{align*}
> $$
>
> 으로 $f(Y\_n)\overset{p}{\to}f(Y)$임을 증명하였다.

<br>

Convergence in probability도 어떤 probability measure 아래에서 따지냐에 따라 달라질 수 있다. 그러므로 어떤 probability distribution의 family $\mathcal{P} = \\{P\_{\theta}: \theta\in\Omega\\}$가 parameter $\theta$를 가지고 있을 때, distribution $P_{\theta}$ 아래에서의 convergence in probability를 표현할 때에는 $\overset{P\_{\theta}}{\to}$의 기호를 사용할 것이다.

이제 frequentist가 estimator의 자질로써 가장 중요하게 생각하는 또다는 property 하나를 다룰 것인데, 바로 *consistency*<sub>일치성</sub>이다.

<br>

> **Definition 19.6** $g(\theta)$의 sequence of estimators $\delta\_n$($n\ge1$)가 *$g(\theta)$에 대하여 consistent*하다는 것은, 모든 $\theta\in\Omega$에 대하여 
>
> $$ \delta_n \overset{P_{\theta}}{\to} g(\theta)\quad \text{as}\quad $n\to\infty$$
>
> 가 성립함을 의미한다.

<br>

Sample mean도 $\mu$에 대한 unbiased estimator라고 볼 수 있는데, 이 역시 consistency를 만족함을 **Example 19.5**에서 알 수 있다. 그러므로 sample mean은 $\mu$에 대한 꽤나 performance가 좋은 estimator라고 볼 수 있다.

Squared error loss 아래에서 risk $R(\theta, \delta\_n) = E\_{\theta}(\delta\_n - g(\theta))^2$을 보통 $\delta\_n$에 따른 *mean squared error*라고 자주 표현한다. 그러면 우리는 consistency와 risk와의 관계를 찾을 수 있는데, 만약 $n\to\infty$임에 따라

$$ R(\theta, \delta_n)\to 0, \quad\forall \theta\in\Omega $$

를 만족하면 **Proposition 19.3**에 따라 $\delta\_n$은 consistent하게 된다. 또, bias와 variance와의 관계도 찾아보면 bias의 제곱과 variance를 더한 것이 risk가 되므로, $n\to\infty$임에 따라 모든 $\theta\in\Omega$에 대하여 bias $b\_n(\theta)\to0$이고 variance $Var\_{\theta}(\delta\_n)\to 0$이면 $\delta\_n$이 consistent하다. 그만큼 consistency를 만족시키기 위한 충분조건이 상당히 강함을 알 수 있다.

Convergence in probability의 개념은 어렵지 않게 더 높은 차원으로 확장시킬 수 있다. $Y, Y\_1, Y\_2, \cdots$를 $\mathbb{R}^p$에 속한 random vector라고 할 때, $Y\_n$이 $Y$로 convergent in probability하다는 것은, 모든 $\epsilon >0$에 대하여

$$ P(\Vert Y_n - Y\vert \gt \epsilon) \to 0 \quad \text{as}\quad n\to \infty $$

표기할 때는 동일하게 $Y\_n\overset{p}{\to} Y$로 한다. 정의로부터 우리는 각 element가 convergent in probability하면, 즉 각 $i = 1, 2, \cdots, p$에 대하여 $n\to\infty$임에 따라 $[Y\_n]\_i\overset{p}{\to} [Y]\_i $이면, $Y\_n\overset{p}{\to} Y$이다. 이는 norm의 성질을 이용하면 쉽게 증명이 가능하다.

이 역시 $c$에서 continuous한 함수 $f:\mathbb{R}^p\to\mathbb{R}^q$에 대하여 $Y\_n\overset{p}{\to} Y$이면 $f(Y\_n)\overset{p}{\to}f(c)$가 성립한다. 그러므로 $f(x, y) = x+y$ 혹은 $f(x, y) = xy$를 적용하면, 만약 $n\to\infty$임에 따라 $X\_n\overset{p}{\to}a$이고 $Y\_n\overset{p}{\to}b$가 성립하면, $n\to\infty$임에 따라

$$
\begin{align*}
X_n + Y_n &\overset{p}{\to} a + b \\
X_nY_n &\overset{p}{\to} ab
\end{align*}
$$

가 성립한다.